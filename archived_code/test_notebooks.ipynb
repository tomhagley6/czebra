{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large snippets of code from test_notebooks folder, for archiving and reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for eye position and a CEBRA-Time model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data for eye position and a CEBRA-Time model ##\n",
    "\n",
    "# for a single fish\n",
    "filename = FILENAME\n",
    "filename_trunc = filename.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = 'data/'\n",
    "\n",
    "# choose where in dataset to sample\n",
    "start, stop = 0, 0+TIMESTEPS\n",
    "\n",
    "\n",
    "# extract eye position and neural data\n",
    "# do not attempt to load the entire file \n",
    "print(\"Accessing data...\")\n",
    "with h5py.File(filename, 'r') as f:\n",
    "\n",
    "    # eye position\n",
    "    eye_pos_l = f['visuomotor']['eye_pos']['Left']\n",
    "    eye_pos_r = f['visuomotor']['eye_pos']['Right']\n",
    "    print(f\"Full eye position dataset shape is: {eye_pos_l.shape}\")\n",
    "\n",
    "    # neural \n",
    "    neural = f['rois']['dfof']\n",
    "    print(f\"Full neural dataset shape is: {neural.shape}\")\n",
    "\n",
    "    # select first TIMESTEPS timesteps and random ROIS rois\n",
    "    # neural\n",
    "    neural_indexes = np.sort(\n",
    "                        np.random.choice(\n",
    "                                    np.arange(neural.shape[1]), size=ROIS, replace=False\n",
    "                                    )\n",
    "                        )\n",
    "    neural = np.array(neural[start:stop, neural_indexes])\n",
    "\n",
    "    # eye position\n",
    "    eye_pos_l = np.array(eye_pos_l[start:stop])\n",
    "    eye_pos_r = np.array(eye_pos_r[start:stop])\n",
    "\n",
    "    print(f\"Truncated dataset shapes are:\\n \\\n",
    "            eye_pos_l: {eye_pos_l.shape}\\n \\\n",
    "            eye_pos_r: {eye_pos_r.shape}\\n \\\n",
    "            neural: {neural.shape}\")\n",
    "\n",
    "    assert(neural.shape == (TIMESTEPS, ROIS))\n",
    "\n",
    "\n",
    "    # save datasets\n",
    "    filename_eye_pos = f'{filename[-12:-3]}_eye_pos.npz'\n",
    "    filename_dfof = f'{filename[-12:-3]}_dfof.npz'\n",
    "    np.savez(f'{data_folder} + {filename_eye_pos}', eye_pos_l=eye_pos_l, eye_pos_r=eye_pos_r)\n",
    "    np.savez(f'{data_folder} + {filename_dfof}', neural=neural)\n",
    "\n",
    "print(\"Data accessed.\")\n",
    "\n",
    "eye_pos_l = cebra.load_data(f'{data_folder}{filename_eye_pos}', key=\"eye_pos_l\")\n",
    "print(f\"{filename_eye_pos}_left loaded.\")\n",
    "eye_pos_r = cebra.load_data(filename_eye_pos, key=\"eye_pos_r\")\n",
    "print(f\"{filename_eye_pos}_right loaded.\")\n",
    "neural = cebra.load_data(f'{data_folder}{filename_eye_pos}', key=\"neural\")\n",
    "print(f\"{filename_dfof} loaded.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### truncate nueral data to be a random selection of ROIs and a consecutive sequence of timesteps, with predefined shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## truncate nueral data to be a random selection of ROIs and a consecutive sequence of timesteps, with predefined shape ##\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# truncate neural\n",
    "# select first TIMESTEPS timesteps and random ROIS rois\n",
    "neural_indexes = np.sort(\n",
    "                    np.random.choice(\n",
    "                                np.arange(neural.shape[1]), size=ROIS, replace=False\n",
    "                                )\n",
    "                    )\n",
    "neural = np.array(neural[start:stop, neural_indexes])\n",
    "print(f'Truncated neural dataset shape is: {neural.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### singular (non-loop) implementation of creating binary masks for stimulus 'on' frames ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## singular (non-loop) implementation of creating binary masks for stimulus 'on' frames ##\n",
    "\n",
    "import h5py\n",
    "\n",
    "filepath = '/media/storage/DATA/lfads_export/f1_221027.h5'\n",
    "stim_types = {'left_spot':0, 'right_spot':1,  \\\n",
    "              'open_loop_grating':2, 'closed_loop_grating':3}\n",
    "neural = np.zeros((100000,1))\n",
    "\n",
    "with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "    # get stimulus presentations\n",
    "    stimuli = f['visuomotor']['presentations']\n",
    "    stim_type = stimuli['stim_type'].astype(int)\n",
    "    stim_onset_fr = stimuli['onset_frame'].astype(int)\n",
    "    stim_end_fr = stimuli['offset_frame'].astype(int)\n",
    "\n",
    "\n",
    "    # find the presentation indexes with left or right spots\n",
    "    stim_pres_idx_l = np.where(np.isin(stim_type, 1))[0]    # left spots\n",
    "    stim_pres_idx_r = np.where(np.isin(stim_type, 2))[0]    # right spots\n",
    "\n",
    "    # index stim onset frames with the presentation indexes\n",
    "    stim_onset_fr = stimuli['onset_frame'].astype(int)\n",
    "    stim_pres_fr_l = stim_onset_fr[stim_pres_idx_l]\n",
    "    stim_pres_fr_r = stim_onset_fr[stim_pres_idx_r]\n",
    "\n",
    "    # index stim end frames with the presentation indexes\n",
    "    stim_end_fr = stimuli['offset_frame'].astype(int)\n",
    "    stim_end_fr_l = stim_end_fr[stim_pres_idx_l]\n",
    "    stim_end_fr_r = stim_end_fr[stim_pres_idx_r]\n",
    "\n",
    "    # create masks of stim onset/stim end\n",
    "    # left spot\n",
    "    stim_on_l = np.zeros(neural.shape[0]).astype(int)\n",
    "    stim_on_l[[stim_pres_fr_l, stim_end_fr_l]] = 1\n",
    "    np.bitwise_xor.accumulate(stim_on_l) | stim_on_l\n",
    "    # right spot\n",
    "    stim_on_r = np.zeros(neural.shape[0]).astype(int)\n",
    "    stim_on_r[[stim_pres_fr_r, stim_end_fr_r]] = 1\n",
    "    np.bitwise_xor.accumulate(stim_on_r) | stim_on_r\n",
    "\n",
    "    # find duration (in frames) of each presentation\n",
    "    # (neural recording is at 5Hz)\n",
    "    stim_dur_l = stim_end_fr_l - stim_pres_fr_l\n",
    "    stim_dur_r = stim_end_fr_r - stim_pres_fr_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-loop version of the stimulus on frame mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING: Here is the full structure as a template for the cell above. Delete when above is implemented\n",
    "# # load data (single fish)\n",
    "\n",
    "# paths\n",
    "filepath = FILEPATH\n",
    "filename = filepath.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = 'data/'\n",
    "filename_spot_pres_fr = f'{filename[-9:]}_spot_pres_fr.npz'\n",
    "filename_dfof = f'{filename[-9:]}_dfof_stim_decode.npz'\n",
    "\n",
    "# choose where in dataset to sample\n",
    "start, stop = 0, 0+TIMESTEPS\n",
    "\n",
    "# extract eye position and neural data\n",
    "# do not attempt to load the entire file \n",
    "print(\"Accessing data...\")\n",
    "\n",
    "# load data if it is already saved, and LOAD == True\n",
    "if LOAD == True:\n",
    "    try:\n",
    "        spot_pres_fr = cebra.load_data(f'{data_folder}{filename_spot_pres_fr}', key=\"spot_pres_fr\")\n",
    "        print(f\"{filename_spot_pres_fr}_left loaded.\")\n",
    "        spot_pres_fr = cebra.load_data(f'{data_folder}{filename_spot_pres_fr}', key=\"spot_pres_fr\")\n",
    "        print(f\"{filename_spot_pres_fr}_right loaded.\")\n",
    "        neural = cebra.load_data(f'{data_folder}{filename_dfof}', key=\"neural\")\n",
    "        print(f\"{filename_dfof} loaded.\")\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "        print(\"Couldn't load data into CEBRA\")\n",
    "\n",
    "else:\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "        # neural\n",
    "        neural = f['rois']['dfof']\n",
    "        print(f\"Full neural dataset shape is: {neural.shape}\")\n",
    "\n",
    "        \n",
    "        # get stimulus presentations\n",
    "        stimuli = f['visuomotor']['presentations']\n",
    "        stim_type = stimuli['stim_type'].astype(int)\n",
    "\n",
    "        for stim in STIMS:\n",
    "\n",
    "\n",
    "\n",
    "        # find the presentation indexes with left or right spots\n",
    "        stim_pres_idx_l = np.where(np.isin(stim_type, 1))[0]    # left spots\n",
    "        stim_pres_idx_r = np.where(np.isin(stim_type, 2))[0]    # right spots\n",
    "\n",
    "        # print spot information\n",
    "        print(f'Out of a total {stim_type.size} stimulus presentations:\\n \\\n",
    "        {spot_pres_fr_l.size} left spots\\n \\\n",
    "        {spot_pres_fr_r.size} right spots')\n",
    "\n",
    "        # index stim onset frames with the presentation indexes\n",
    "        stim_onset_fr = stimuli['onset_frame'].astype(int)\n",
    "        stim_pres_fr_l = stim_onset_fr[stim_pres_idx_l]\n",
    "        stim_pres_fr_r = stim_onset_fr[stim_pres_idx_r]\n",
    "\n",
    "        # index stim end frames with the presentation indexes\n",
    "        stim_end_fr = stimuli['offset_frame'].astype(int)\n",
    "        stim_end_fr_l = stim_end_fr[stim_pres_idx_l]\n",
    "        stim_end_fr_r = stim_end_fr[stim_pres_idx_r]\n",
    "\n",
    "        # create masks of stim onset/stim end\n",
    "        # left spot\n",
    "        stim_on_l = np.zeros(neural.shape[0])\n",
    "        stim_on_l[[stim_pres_fr_l, stim_end_fr_l]] = 1\n",
    "        np.bitwise_xor.accumulate(stim_on_l) | stim_on_l\n",
    "        # right spot\n",
    "        stim_on_r = np.zeros(neural.shape[0])\n",
    "        stim_on_r[[stim_pres_fr_r, stim_end_fr_r]] = 1\n",
    "        np.bitwise_xor.accumulate(stim_on_r) | stim_on_r\n",
    "\n",
    "        # find duration (in frames) of each presentation\n",
    "        # (neural recording is at 5Hz)\n",
    "        stim_dur_l = stim_end_fr_l - stim_pres_fr_l\n",
    "        stim_dur_r = stim_end_fr_r - stim_pres_fr_r\n",
    "        \n",
    "        spot_pres_fr = np.column_stack((spot_pres_frames_l, spot_pres_fr_r))\n",
    "\n",
    "        # assert shapes\n",
    "        assert(neural.shape == (TIMESTEPS, ROIS))\n",
    "        assert(spot_pres_frames.shape == (spot_pres_frames_l.size, 2))\n",
    "\n",
    "        # save data\n",
    "        np.savez(f'{data_folder}{filename_spot_pres_fr}', spot_pres_fr=spot_pres_fr)\n",
    "        np.savez(f'{data_folder}{filename_dfof}', neural=neural)\n",
    "\n",
    "        # load data\n",
    "        spot_pres_fr = cebra.load_data(f'{data_folder}{filename_spot_pres_fr}', key=\"spot_pres_fr\")\n",
    "        print(f\"{filename_spot_pres_fr}_left loaded.\")\n",
    "        neural = cebra.load_data(f'{data_folder}{filename_dfof}', key=\"neural\")\n",
    "        print(f\"{filename_dfof} loaded.\")\n",
    "    \n",
    "\n",
    "print(spot_pres_fr_l)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Older, more convoluted loading step before deciding to load dfof from HDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data for a single fish ###\n",
    "\n",
    "##  params ##\n",
    "\n",
    "# variables\n",
    "stim_types = STIM_TYPES     # dict of all possible stims\n",
    "stims = STIMS               # stim types chosen for analysis\n",
    "\n",
    "# paths\n",
    "filepath = FILEPATH\n",
    "filename = filepath.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = 'data/'\n",
    "data_folder_HDD = '/media/storage/DATA/tom/'\n",
    "filename_stim_pres_frames = f'{filename[-9:]}_stim_pres_frames.npz'\n",
    "filename_dfof = f'{filename[-9:]}_dfof_stim_decode.npz'\n",
    "\n",
    "## loading ##\n",
    "\n",
    "# only generate new data files if LOAD is not specified\n",
    "if LOAD:\n",
    "    try:\n",
    "        # load data\n",
    "        # default to loading directly from hdf5 for large datasets\n",
    "        stim_pres_frames = cebra.load_data(f'{data_folder}{filename_stim_pres_frames}', key=\"stim_pres_frames\")\n",
    "        print(f\"Stimulus presentation frames loaded.\")\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            try:\n",
    "                neural = f['rois']['data']\n",
    "                neural = cebra.load_data(filepath, key='rois/dfof')\n",
    "                print(\"Neural data loaded\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                try:\n",
    "                    neural = cebra.load_data(f'{data_folder_HDD}{filename_dfof}', key=\"neural\")\n",
    "                    print(f\"Loaded all datasets\")\n",
    "\n",
    "                except:\n",
    "                    print(f\"Could not load data.\")\n",
    "    except:\n",
    "        print(f\"Could not load data.\")\n",
    "\n",
    "else:\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "        ## neural ##\n",
    "\n",
    "        neural = f['rois']['dfof']\n",
    "        print(f\"Full neural dataset shape is: {neural.shape}\")\n",
    "\n",
    "        ## stimuli ##\n",
    "\n",
    "        # get stimulus presentations\n",
    "        stimuli = f['visuomotor']['presentations']\n",
    "        stim_type = stimuli['stim_type'].astype(int)\n",
    "        stim_on_fr = stimuli['onset_frame'].astype(int)\n",
    "        stim_end_fr = stimuli['offset_frame'].astype(int)\n",
    "\n",
    "        # initialise lists for the chosen stimuli\n",
    "        (stim_pres_idx_list, stim_on_fr_list,\n",
    "        stim_end_fr_list, stim_on_mask_list, stim_dur_list)  = [],[],[],[],[]\n",
    "\n",
    "\n",
    "        # loop through chosen stimuli and find boolean masks for their 'on' frames\n",
    "        for stim in stims:\n",
    "\n",
    "            # convert stim name to stim number\n",
    "            stim_num = stim_types[stim] \n",
    "            print(f'Attempting to parse stim: {stim}') \n",
    "\n",
    "            # find the presentation indexes for the specified stim type\n",
    "            # must account for data index starting at 1\n",
    "            this_stim_pres_indexes = np.where(np.isin(stim_type, stim_num + 1))[0]\n",
    "            stim_pres_idx_list.append(this_stim_pres_indexes)\n",
    "\n",
    "            # index stim onset frame numbers with the presentation indexes\n",
    "            this_stim_on_frames = stim_on_fr[this_stim_pres_indexes]\n",
    "            stim_on_fr_list.append(this_stim_on_frames)\n",
    "\n",
    "            # index stim end frame numbers with the presentation indexes\n",
    "            this_stim_end_frames = stim_end_fr[this_stim_pres_indexes]\n",
    "            stim_end_fr_list.append(this_stim_end_frames)\n",
    "\n",
    "            # create a boolean mask of stimulus presentation frames (1 == stimulus on, 0 == stimulus off)\n",
    "            this_stim_on_mask = np.zeros(neural.shape[0]).astype(int)\n",
    "            this_stim_on_mask[[this_stim_on_frames, this_stim_end_frames]] = 1\n",
    "            # perform bitwise XOR operation on consecutive elements of stim_on_mask. This will convert all \n",
    "            # but stim_off frame to 1s. Combining with \"OR stim_on_mask\" will also include the stim_off frame\n",
    "            stim_on_mask = np.bitwise_xor.accumulate(this_stim_on_mask) | this_stim_on_mask\n",
    "            stim_on_mask_list.append(this_stim_on_mask)\n",
    "\n",
    "            # find duration (in frames) of each presentation of the stimulus\n",
    "            # recording rate is 5 Hz\n",
    "            stim_dur_list.append(this_stim_end_frames - this_stim_on_frames)\n",
    "\n",
    "            # assert shapes\n",
    "            assert(stim_on_mask_list[0].size == neural.shape[0])\n",
    "\n",
    "            print(f'Stim type {stim} parsed successfully.')\n",
    "\n",
    "        if SAVE:\n",
    "            # save all data as .npz \n",
    "            # (large datasets saved to HDD)\n",
    "            stim_on_mask_dataset = np.column_stack(stim_on_mask_list[:])\n",
    "            np.savez(f'{data_folder}{filename_stim_pres_frames}', stim_pres_frames=stim_on_mask_dataset)\n",
    "            print(f\"Stim presentation dataset saved.\")\n",
    "            np.savez(f'{data_folder_HDD}{filename_dfof}', neural=neural)\n",
    "            print(f\"Neural dataset saved.\")\n",
    "            print(f\"All datasets saved.\")\n",
    "\n",
    "\n",
    "        # load data\n",
    "        # assume all data is saved as .npz\n",
    "        stim_pres_frames_ = cebra.load_data(f'{data_folder}{filename_stim_pres_frames}', key=\"stim_pres_frames\")\n",
    "        print(f\"Stimulus presentation frames loaded.\")\n",
    "        neural = cebra.load_data(f'{data_folder_HDD}{filename_dfof}', key=neural)\n",
    "        print(\"Neural data loaded\")\n",
    "        print(\"All data loaded.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
