{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPIM data CEBRA model\n",
    "\n",
    "- Use CEBRA label contrastive learning on neural data from one fish\n",
    "    - design model\n",
    "    - load data\n",
    "    - SPIM data preprocessing\n",
    "    - fit with label of the stimulus\n",
    "    - plot embeddings\n",
    "    - decode stimulus presence and type (left/right spots)<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, running time-since-last, to analyse stimulus-triggered response\n",
    "\n",
    "### Implementing a 'leaky integrator' continuous representation to replace the linear time-since-last\n",
    "\n",
    "### Updated leaky integrator to remove any data points that overlap with stimulus presentation, and to use the first timepoint post-stimulus as the convolution trigger\n",
    "\n",
    "### Updated again to include both left and right stimulus predictions, and feed these into the same analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomh/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1682343962757/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib ipympl\n",
    "import cebra\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set random seeds ###\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define globals ### \n",
    "\n",
    "# list of all data files\n",
    "dat_files = ['/media/storage/DATA/lfads_export/f1_221027.h5',\n",
    "             '/media/storage/DATA/lfads_export/f1_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f2_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f3_221103.h5']\n",
    "\n",
    "global SAMPLING_TYPE           # either 'time', 'time_delta', or 'delta' (delta minimally implemented)\n",
    "global LABEL                   # brief description of model training label\n",
    "global EXTRA_LABEL             # optional extra information to include in model name\n",
    "global FILEPATH                # path to data file\n",
    "global DATA_PATH               # path to experimental data\n",
    "global MODELPATH_ROOT          # folder path for models folder\n",
    "global MODELPATH_DATE          # specific path within models folder\n",
    "global MODEL_VER               # append version to model name for when saving multiple repeats\n",
    "global FRAMERATE               # framerate of neural recording, Hz\n",
    "global TIMESTEPS               # no. of neural timepoints to use\n",
    "global ROIS                    # no. of ROIs to use \n",
    "global ITERS                   # model training iterations\n",
    "global PARTIAL_FIT             # train the model in specified incremental step. As the model can be saved and loaded as\n",
    "                               # usual, can be combined with LOAD_MODEL to repeatedly train\n",
    "global LOAD_DATA               # attempt to load pre-saved .npz data files\n",
    "global SAVE_DATA               # save new .npz files when created\n",
    "global LOAD_MODEL              # attempt to load model\n",
    "global SAVE_MODEL              # save model to models folder\n",
    "global SAVE_FIGS               # automatically save figures generated\n",
    "global SAVE_FIGS_LABEL         # extra label to be added to figure names as they are saved\n",
    "global STIM_TYPES              # dictionary of stim types to stim numbers\n",
    "global STIMS                   # relevant stims for model\n",
    "global TRAINING_TEST_SPLIT     # split fraction for embedding model\n",
    "global STIM_LENGTH_FRAMES      # the length in frames of stimuli, assuming all stimuli have equal duration\n",
    "global HALF_STIM_MASK          # only use the second half of stimuli for training the model\n",
    "global DISCRETE_AND_CONT_STIM  # combine the discrete stimulus identifier variable with an linearly increasing \n",
    "                               # continuous variable.\n",
    "                               # continuous variable is currently independent of other flags. It will e.g. not \n",
    "                               # be influenced by HALF_STIM_MASK\n",
    "global PREDICT_ON_TRAIN        # for decoder, do both training and prediciton on the training set\n",
    "global RESTRICT_TO_TECTAL      # get random subset of neurons from the tectal mask only\n",
    "global TRIAL_TIME              # If True, use concatenated peri-trial timeseries instead of continuous timeseries\n",
    "global SIGNAL_TYPE             # whether to use deconvolved calcium or raw calcium\n",
    "global RANDOMISED_TEST         # when running on discontinuous data, randomise the order of the 'trials' for a \n",
    "                               # balanced test set\n",
    "global TIME_SINCE_LAST         # continuous variable recording the time since last stimulus (of that type).\n",
    "                               # not compatible with the discrete and continuous stim labelling\n",
    "global LEAKY_INTEGRATOR        # continuous variable convolving a decaying exponential with stimulus onset frames\n",
    "\n",
    "SAMPLING_TYPE = 'time_delta'\n",
    "LABEL = 'spot'                                                          # set as '' for time-contrastive\n",
    "EXTRA_LABEL = 'second-half-stims_leaky-integrator_tectal_double-predictions_learning_rate_change'\n",
    "FILEPATH = dat_files[1]\n",
    "DATA_PATH = '/home/tomh/Documents/projects/czebra/test_notebooks/data/'\n",
    "MODELPATH_ROOT = '/home/tomh/Documents/projects/czebra/archived_models/'\n",
    "MODELPATH_DATE = '231030/'\n",
    "MODEL_VER = 8\n",
    "FRAMERATE = 5\n",
    "TIMESTEPS = None                                                        # set as None to use all timepoints\n",
    "ROIS =  10000 #int(101435 * 0.50) \n",
    "ITERS = 500 # 500\n",
    "PARTIAL_FIT = False                                                     # set False to ignore, else max_iters\n",
    "LOAD_DATA = True\n",
    "LOAD_MODEL = True\n",
    "SAVE_DATA = False\n",
    "SAVE_MODEL = False\n",
    "SAVE_FIGS = True\n",
    "STIM_TYPES = {'left_spot':0, 'right_spot':1,  \\\n",
    "              'open_loop_grating':2, 'closed_loop_grating':3}\n",
    "STIMS = ['left_spot', 'right_spot']\n",
    "TRAINING_TEST_SPLIT = 0.8\n",
    "STIM_LENGTH_FRAMES = 22\n",
    "HALF_STIM_MASK = True\n",
    "DISCRETE_AND_CONT_STIM = False     \n",
    "PREDICT_ON_TRAIN = True\n",
    "RESTRICT_TO_TECTAL = True\n",
    "SAVE_FIGS_LABEL = 'predict-on-train_' if SAVE_FIGS and PREDICT_ON_TRAIN else ''\n",
    "TRIAL_TIME = False\n",
    "SIGNAL_TYPE = 'deconv'        # dfof for raw, deconv for deconvolved\n",
    "RANDOMISED_TEST = False\n",
    "TIME_SINCE_LAST = False\n",
    "LEAKY_INTEGRATOR = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading data, the notebook will try to load the generic dataset with the name given by the chosen combination of flags. This is similar for loading the model, but it will also take into account the \"extra label\", to string match to the right model folder. Remember to set the extra label to match the chosen flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEBRA(batch_size=128, conditional='time_delta', learning_rate=1e-06,\n",
      "      max_iterations=500, model_architecture='offset10-model',\n",
      "      num_hidden_units=128,\n",
      "      optimizer_kwargs=(('betas', (0.9, 0.999)), ('eps', 1e-08),\n",
      "                        ('weight_decay', 0.001), ('amsgrad', False)),\n",
      "      output_dimension=3, temperature_mode='auto', time_offsets=10,\n",
      "      verbose=True)\n"
     ]
    }
   ],
   "source": [
    "### define model ###\n",
    "\n",
    "parameters = {'conditional':SAMPLING_TYPE,\n",
    "              'max_iterations':ITERS,\n",
    "              'batch_size':128,\n",
    "              'num_hidden_units':128,\n",
    "              'weight_decay':1e-03,\n",
    "              'rois':ROIS,\n",
    "             }\n",
    "\n",
    "cebra_stim_on_model = cebra.CEBRA(\n",
    "    model_architecture='offset10-model',\n",
    "    device='cuda_if_available',\n",
    "    conditional=parameters['conditional'],\n",
    "    temperature_mode='auto',\n",
    "    min_temperature=0.1,\n",
    "    time_offsets=10,\n",
    "    max_iterations= parameters['max_iterations'] if not PARTIAL_FIT else PARTIAL_FIT,\n",
    "    max_adapt_iterations=500,\n",
    "    batch_size=parameters['batch_size'],\n",
    "    learning_rate=1e-6,\n",
    "    output_dimension=3,\n",
    "    verbose=True,\n",
    "    num_hidden_units=parameters['num_hidden_units'],\n",
    "    hybrid=False,\n",
    "    optimizer_kwargs=(('betas', (0.9, 0.999)), ('eps', 1e-08), ('weight_decay', parameters['weight_decay']), ('amsgrad', False))\n",
    "    )\n",
    "print(cebra_stim_on_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for loading, saving and preprocessing model input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neural_dataset(f, neural, rois, timesteps=None, restrict_to_tectal=None):  \n",
    "    ''' Take a random sample of rois from either the tectal-restricted dataset or\n",
    "        the full dataset. \n",
    "        If a number of timesteps are given, only index the neural data up to this value\n",
    "        f                   - h5pyFile object for loading\n",
    "        neural              - neural dataset from above file\n",
    "        rois                - num rois to choose\n",
    "        timesteps           - maximum index of neural in time dimension\n",
    "        restrict_to_tectal  - flag for restricting dataset to tectum only'''\n",
    "    \n",
    "    # decide whether to sample from all neurons, or tectal neurons only\n",
    "    if restrict_to_tectal:\n",
    "        \n",
    "        print(\"Taking tectal neurons only.\")\n",
    "        tectal_mask = f['rois']['anat_label']['tectum_SPV_AZ'][:]\n",
    "        neural_indexes = np.sort(\n",
    "                            np.random.choice(\n",
    "                                            np.where(tectal_mask)[0], size=rois, replace=False\n",
    "                                            )\n",
    "                                )\n",
    "    else:\n",
    "        \n",
    "        # subset neural by selecting first TIMESTEPS timesteps and random ROIS rois\n",
    "        neural_indexes = np.sort(\n",
    "                            np.random.choice(\n",
    "                                        np.arange(neural.shape[1]), size=rois, replace=False\n",
    "                                        )\n",
    "                            )\n",
    "    \n",
    "    # if a subset of timesteps is chosen, restrict data to this subset\n",
    "    if timesteps:\n",
    "        neural = np.array(neural[:timesteps, neural_indexes])\n",
    "    else:\n",
    "        neural = np.array(neural[:, neural_indexes])\n",
    "        timesteps = neural.shape[0]\n",
    "\n",
    "    print(f\"Truncated dataset size: {neural.shape}\")\n",
    "    assert(neural.shape == (timesteps, rois))\n",
    "\n",
    "    return (neural, neural_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stimulus_presentation_masks(f, neural, stims, stim_types, stim_length_frames, timesteps=None):\n",
    "    ''' For every input stimulus type, find:\n",
    "    - index value of the stimulus presentations\n",
    "    - frame index of stimulus onset\n",
    "    - frame index of stimulus end\n",
    "    - the boolean mask for when this stimulus is being presented\n",
    "    - the above 3 for the second half of the stimulus duration\n",
    "    - the stimulus duration\n",
    "      \n",
    "    Second-half stimulus found by adding half of stimulus duration to onset frame numbers\n",
    "    f                   - h5pyFile object for loading\n",
    "    neural              - neural dataset from above file\n",
    "    stims               - chosen stim types for this extraction (ArrayLike of ints for indexing stim_types)\n",
    "    stim_types          - dict of stimulus types, indexed by stims\n",
    "    stim_length_frames  - duration of stimuli (currently fixed for all stimuli)\n",
    "    timesteps           - maximum index of neural in time dimension\n",
    "    '''\n",
    "\n",
    "\n",
    "     # get stimulus presentations\n",
    "    stimuli = f['visuomotor']['presentations']\n",
    "    stim_type = stimuli['stim_type'][:timesteps].astype(int)\n",
    "    stim_on_fr = stimuli['onset_frame'][:timesteps].astype(int)\n",
    "    stim_end_fr = stimuli['offset_frame'][:timesteps].astype(int)\n",
    "\n",
    "    # initialise lists for the chosen stimuli\n",
    "    (stim_pres_idx_list, stim_on_fr_list, \n",
    "    stim_on_fr_list_half,\n",
    "    stim_end_fr_list, stim_on_mask_list, \n",
    "    stim_on_mask_list_half,\n",
    "    stim_dur_list)                        = [],[],[],[],[],[],[]\n",
    "\n",
    "\n",
    "    # loop through chosen stimuli and find boolean masks for their 'on' frames\n",
    "    for stim in stims:\n",
    "\n",
    "        # convert stim name to stim number\n",
    "        stim_num = stim_types[stim] \n",
    "        print(f'Attempting to parse stim: {stim}') \n",
    "\n",
    "        # find the presentation indexes for the specified stim type\n",
    "        # must account for data index starting at 1\n",
    "        this_stim_pres_indexes = np.where(np.isin(stim_type, stim_num + 1))[0]\n",
    "        stim_pres_idx_list.append(this_stim_pres_indexes)\n",
    "\n",
    "        # index stim-specific onset frame numbers \n",
    "        this_stim_on_frames = stim_on_fr[this_stim_pres_indexes]\n",
    "        if timesteps:\n",
    "            this_stim_on_frames = this_stim_on_frames[this_stim_on_frames < timesteps]\n",
    "        stim_on_fr_list.append(this_stim_on_frames)\n",
    "\n",
    "        # Find also the second half of stimulus frames: increment the start frame\n",
    "        # indexes by half the stimulus duration, to simulate the stimulus starting halfway\n",
    "        # later than it actually does\n",
    "        this_stim_on_frames_half = (this_stim_on_frames + stim_length_frames/2).astype(int)\n",
    "        stim_on_fr_list_half.append(this_stim_on_frames_half)\n",
    "\n",
    "        # index stim-specific end frame numbers \n",
    "        this_stim_end_frames = stim_end_fr[this_stim_pres_indexes]\n",
    "        if timesteps:\n",
    "            this_stim_end_frames = this_stim_end_frames[this_stim_end_frames < timesteps]\n",
    "        stim_end_fr_list.append(this_stim_end_frames)\n",
    "            \n",
    "\n",
    "        # create a boolean mask of stimulus presentation frames (1 == stimulus on, 0 == stimulus off)\n",
    "        # do this for second-half-stim and full stim\n",
    "        this_stim_on_mask = np.zeros(neural.shape[0]).astype(int)\n",
    "        this_stim_on_mask[[this_stim_on_frames, this_stim_end_frames]] = 1\n",
    "        this_stim_on_mask_half = np.zeros(neural.shape[0]).astype(int)\n",
    "        this_stim_on_mask_half[[this_stim_on_frames_half, this_stim_end_frames]] = 1\n",
    "        \n",
    "        # perform bitwise XOR operation on consecutive elements of stim_on_mask. This will convert all \n",
    "        # but stim_off frame to 1s. Combining with \"OR stim_on_mask\" will also include the stim_off frame\n",
    "        this_stim_on_mask = np.bitwise_xor.accumulate(this_stim_on_mask) | this_stim_on_mask\n",
    "        stim_on_mask_list.append(this_stim_on_mask)\n",
    "        this_stim_on_mask_half = np.bitwise_xor.accumulate(this_stim_on_mask_half) | this_stim_on_mask_half\n",
    "        stim_on_mask_list_half.append(this_stim_on_mask_half)\n",
    "        \n",
    "        # find duration (in frames) of each presentation of the stimulus\n",
    "        # recording rate is 5 Hz\n",
    "        stim_dur_list.append(this_stim_end_frames - this_stim_on_frames)\n",
    "\n",
    "        # assert shapes\n",
    "        print(f\"Label shape: {stim_on_mask_list[0].shape}\")\n",
    "        print(f'Stim type {stim} parsed successfully.')\n",
    "\n",
    "    return (stim_pres_idx_list, stim_on_fr_list, stim_on_fr_list_half,\n",
    "            stim_end_fr_list, stim_on_mask_list, stim_on_mask_list_half,\n",
    "                stim_dur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_file(stim_on_mask_dataset, neural, neural_indexes, \n",
    "              data_folder, filename_stim_pres_frames, filename_neural_subset,\n",
    "              filename_neural_indexes):   \n",
    "    ''' Save data to the specified folder. This includes neural data, indexes of \n",
    "        neural ROIs, and a binary mask of stimulus activity\n",
    "        stim_on_mask_dataset    - binary mask of stimulus activity (all stims)\n",
    "        neural                  - neural dataset from above file\n",
    "        neural_indexes          - index values of rois used in the dataset (w.r.t raw data)\n",
    "        data folder              - maximum index of neural in time dimension\n",
    "        filename_stim_pres_frames - flag for restricting dataset to tectum only\n",
    "        data folder              - maximum index of neural in time dimension\n",
    "        filename_stim_pres_frames - flag for restricting dataset to tectum only\n",
    "    '''\n",
    "\n",
    "    np.savez(f'{data_folder}{filename_stim_pres_frames}', stim_on_frames=stim_on_mask_dataset)\n",
    "    print(f\"Stim presentation dataset saved.\")\n",
    "    np.savez(f\"{data_folder}{filename_neural_subset}\", neural=neural)\n",
    "    print(f\"Neural dataset saved.\")\n",
    "    np.savez(f\"{data_folder}{filename_neural_indexes}\", neural_indexes=neural_indexes)\n",
    "    print(f\"All datasets saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(filepath, data_folder, filename_neural_subset,\n",
    "                        filename_stim_pres_frames):\n",
    "    ''' Attempt to load .npz files for neural data and auxiliary variables'''\n",
    "    \n",
    "    key = \"neural\"\n",
    "    try:\n",
    "        # loading dataset from SSD (not HDD) here\n",
    "        neural = cebra.load_data(f\"{data_folder}{filename_neural_subset}\", key=key)\n",
    "        print(\"Neural data loaded\")\n",
    "\n",
    "    except:\n",
    "        print(f\"Could not neural data from file {data_folder}{filename_neural_subset} under key {key}.\")\n",
    "        try:\n",
    "            # try loading directly from the .h5 file\n",
    "            neural = cebra.load_data(filepath, key='rois/deconv')\n",
    "            print(\"Full neural data loaded from HDD file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # give up\n",
    "            print(\"Could not load data.\")\n",
    "            print(e)\n",
    "    \n",
    "    # auxiliary variables\n",
    "    stim_on_frames = cebra.load_data(f'{data_folder}{filename_stim_pres_frames}', key=\"stim_on_frames\")\n",
    "    print(f\"Stimulus presentation frames loaded.\")\n",
    "    print(\"All data loaded.\")\n",
    "\n",
    "    return (neural, stim_on_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing data...\n",
      "Loading data...\n",
      "Neural data loaded\n",
      "Stimulus presentation frames loaded.\n",
      "All data loaded.\n"
     ]
    }
   ],
   "source": [
    "### load and preprocess data for a single fish ###\n",
    "# if LOAD == True, load pre-saved .npz file data. Otherwise,\n",
    "# create this data as specified below and save it to .npz\n",
    "\n",
    "##  params ##\n",
    "\n",
    "# variables\n",
    "stim_types = STIM_TYPES     # dict of all possible stims\n",
    "stims = STIMS               # stim types chosen for analysis\n",
    "timesteps = TIMESTEPS\n",
    "rois = ROIS\n",
    "stim_length_frames = STIM_LENGTH_FRAMES # used for selecting the second half of stimuli\n",
    "\n",
    "load_data = LOAD_DATA\n",
    "save_data = SAVE_DATA\n",
    "\n",
    "# paths\n",
    "filepath = FILEPATH\n",
    "filename = filepath.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = DATA_PATH\n",
    "data_folder_HDD = '/media/storage/DATA/tom/'\n",
    "filename_stim_pres_frames = f'{filename[-9:]}_stim_pres_frames.npz'\n",
    "filename_neural_subset = f'{filename[-9:]}_{SIGNAL_TYPE}_subset.npz'\n",
    "filename_neural_indexes = f'{filename[-9:]}_neural_indexes_all.npz'\n",
    "\n",
    "# specify loading anatomically unrestricted data or tectal-restricted data\n",
    "if RESTRICT_TO_TECTAL:\n",
    "    filename_neural_subset = f'{filename[-9:]}_{SIGNAL_TYPE}_subset_tectal.npz'\n",
    "    filename_neural_indexes = f'{filename[-9:]}_neural_indexes_tectal.npz'\n",
    "\n",
    "# if not loading data, but not wanting to overwrite saved data, save as a temp file\n",
    "if not save_data and not load_data: \n",
    "    print(f\"Producing temp files...\")\n",
    "    filename_neural = f'{filename[-9:]}_{SIGNAL_TYPE}_TEMPORARY_DELETE.npz'\n",
    "    filename_neural_subset = f'{filename[-9:]}_{SIGNAL_TYPE}_subset_TEMPORARY_DELETE.npz'\n",
    "    filename_stim_pres_frames = f'{filename[-9:]}_stim_pres_frames_TEMPORARY_DELETE.npz'\n",
    "\n",
    "\n",
    "print(\"Accessing data...\")\n",
    "\n",
    "## load data ##\n",
    "if load_data:\n",
    "        \n",
    "    # Attempt to load neural data from .npz, otherwise load from HDD .h5\n",
    "    # Load small datasets from .npz files\n",
    "    print(\"Loading data...\")\n",
    "    (neural, stim_on_frames) =  load_data_from_file(filepath, data_folder, filename_neural_subset,\n",
    "                                                    filename_stim_pres_frames)\n",
    "\n",
    "\n",
    "## else generate data ##\n",
    "else:\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "            ## neural ##\n",
    "\n",
    "            neural_dataset = f['rois'][f'{SIGNAL_TYPE}']\n",
    "            print(f\"Full neural dataset shape is: {neural_dataset.shape}\")\n",
    "\n",
    "            neural, neural_indexes = generate_neural_dataset(f, neural_dataset, rois, timesteps=timesteps,\n",
    "                                                             restrict_to_tectal=RESTRICT_TO_TECTAL)\n",
    "\n",
    "            ## stimuli ##\n",
    "\n",
    "            (stim_pres_idx_list, \n",
    "             stim_on_fr_list,\n",
    "             stim_on_fr_list_half,\n",
    "             stim_end_fr_list, \n",
    "             stim_on_mask_list, \n",
    "             stim_on_mask_list_half,\n",
    "             stim_dur_list) = create_stimulus_presentation_masks(f, neural, stims, stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "\n",
    "            ## save data ##\n",
    "            print(\"Saving data...\")\n",
    "\n",
    "            # choose which stim_on_mask to use (half or full)\n",
    "            # stim_on_mask half will ignore the first half of stimuli for the rest\n",
    "            # of the session\n",
    "            if HALF_STIM_MASK:\n",
    "                stim_on_mask_dataset = np.column_stack(stim_on_mask_list_half[:])\n",
    "            else: \n",
    "                 stim_on_mask_dataset = np.column_stack(stim_on_mask_list[:])\n",
    "\n",
    "            assert(stim_on_mask_dataset.shape == (neural.shape[0], len(stims)))\n",
    "            if timesteps:\n",
    "                assert(neural.shape == (timesteps, rois))\n",
    "\n",
    "            save_data_to_file(stim_on_mask_dataset, neural, neural_indexes, \n",
    "              data_folder, filename_stim_pres_frames, filename_neural_subset,\n",
    "              filename_neural_indexes)\n",
    "\n",
    "            ## load data ##\n",
    "            # Attempt to load neural data from .npz, otherwise load from HDD .h5\n",
    "            # Load small datasets from .npz files\n",
    "            print(\"Loading data...\")\n",
    "            \n",
    "            (neural, stim_on_frames) = load_data_from_file(filepath, data_folder, filename_neural_subset,\n",
    "                                                           filename_stim_pres_frames)\n",
    "# end else   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating continuous 'leaky integrator' variable for stimuli...\n",
      "Attempting to parse stim: left_spot\n",
      "Label shape: (38550,)\n",
      "Stim type left_spot parsed successfully.\n",
      "Attempting to parse stim: right_spot\n",
      "Label shape: (38550,)\n",
      "Stim type right_spot parsed successfully.\n",
      "Convolution complete.\n",
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "### data processing ###\n",
    "\n",
    "# format the discrete variable\n",
    "# left spot == 1, right spot == 2, no stimulus == 0 \n",
    "left_spot, right_spot = stim_on_frames[:,0], stim_on_frames[:,1]\n",
    "right_spot = np.multiply(right_spot, 2)\n",
    "discrete = np.add(left_spot, right_spot)\n",
    "\n",
    "# separate data into training and test\n",
    "training_test_split = TRAINING_TEST_SPLIT\n",
    "split_idx = int(np.round(neural.shape[0] * training_test_split))\n",
    "neural_train, neural_test = neural[:split_idx, :], neural[split_idx:, :]\n",
    "discrete_train, discrete_test = discrete[:split_idx], discrete[split_idx:]\n",
    "\n",
    "# if set, create a separate continuous \"contrast\" variable for each stimulus, to train\n",
    "# the CEBRA model on alongside the discrete variable\n",
    "# This is INDEPENDENT of other flags. It will always give the true linear ramp from \n",
    "# stimulus onset to end\n",
    "if DISCRETE_AND_CONT_STIM:\n",
    "    \n",
    "    print(f\"Creating continuous 'contrast' variable for stimuli...\")\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        (_,stim_on_fr_list,_,_,_,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                        stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "        \n",
    "    left_spot_cont = np.zeros(neural.shape[0])\n",
    "    right_spot_cont = np.zeros(neural.shape[0])\n",
    "    cont_stimuli = [left_spot_cont, right_spot_cont]\n",
    "    for i in range(len(cont_stimuli)):\n",
    "        this_stim_on_fr = stim_on_fr_list[i]\n",
    "        for pres in this_stim_on_fr:\n",
    "            cont_stimuli[i][pres:pres+STIM_LENGTH_FRAMES] = np.arange(STIM_LENGTH_FRAMES)\n",
    "\n",
    "    # separate data into training and test\n",
    "    left_spot_cont_train, left_spot_cont_test =  left_spot_cont[:split_idx], left_spot_cont[split_idx:]\n",
    "    right_spot_cont_train, right_spot_cont_test = right_spot_cont[:split_idx], right_spot_cont[split_idx:]\n",
    "    \n",
    "    # arange as expected input to CEBRA\n",
    "    cont_test = np.vstack((left_spot_cont_test, right_spot_cont_test)).T\n",
    "    cont_train = np.vstack((left_spot_cont_train, right_spot_cont_train)).T\n",
    "\n",
    "    print(f\"Created.\")\n",
    "\n",
    "# if set, create a new array for each stimulus which counts up the indices since the last presentation\n",
    "# of this stimulus\n",
    "if TIME_SINCE_LAST:\n",
    "\n",
    "    print(f\"Creating continuous 'time-since-last' variable for stimuli...\")\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        (_,stim_on_fr_list,_,_,_,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                        stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "    \n",
    "    # initialise counting arrays\n",
    "    time_since_last_spot_l = np.zeros(discrete.size)\n",
    "    time_since_last_spot_r = np.zeros(discrete.size)\n",
    "    time_since_last_stim_list = [time_since_last_spot_l, time_since_last_spot_r]\n",
    "\n",
    "    # for each stimulus type\n",
    "    for stim in range(len(stim_on_fr_list)):\n",
    "        this_stim_on_fr = stim_on_fr_list[stim]\n",
    "        time_since_last_stim = time_since_last_stim_list[stim]\n",
    "        # loop through each presentation of stim in order\n",
    "        for pres in this_stim_on_fr:\n",
    "            # create counter\n",
    "            counter = np.arange(pres, discrete.size)\n",
    "            # overwrite the counting array with the counter\n",
    "            # this will start a new count at the index of each stimulus presentation,\n",
    "            # and continue counting until the end of the session, unless overwritten later\n",
    "            # in the loop by another stimulus presentation\n",
    "            time_since_last_stim[pres:] = counter - pres\n",
    "\n",
    "    # separate data into training and test\n",
    "    left_spot_cont_train, left_spot_cont_test = time_since_last_spot_l[:split_idx], time_since_last_spot_l[split_idx:]\n",
    "    right_spot_cont_train, right_spot_cont_test = time_since_last_spot_r[:split_idx], time_since_last_spot_r[split_idx:]\n",
    "\n",
    "    ## remove time before the first round of stimulus presentations (as 0 time-since-last \n",
    "    ## doesn't make logical sense\n",
    "    # identify the end of 0 frames\n",
    "    first_stim_pres = [stim_on_fr_list[i][0] for i in range(len(stim_on_fr_list))]\n",
    "    latest_of_first_stim_pres = max(first_stim_pres)\n",
    "\n",
    "    # truncate start of relevant data\n",
    "    # specifically, neural data and stimulus count data (and discrete just in-case)\n",
    "    # time_since_last_stim_list = [time_since[latest_of_first_stim_pres:] for time_since in time_since_last_stim_list]\n",
    "    left_spot_cont_train = left_spot_cont_train[latest_of_first_stim_pres:]\n",
    "    right_spot_cont_train = right_spot_cont_train[latest_of_first_stim_pres:]\n",
    "    neural_train = neural_train[latest_of_first_stim_pres:]\n",
    "    discrete_train = discrete_train[latest_of_first_stim_pres:]\n",
    "\n",
    "\n",
    "    # arange as expected input to CEBRA\n",
    "    cont_test = np.vstack((left_spot_cont_test, right_spot_cont_test)).T\n",
    "    cont_train = np.vstack((left_spot_cont_train, right_spot_cont_train)).T\n",
    "\n",
    "# end if\n",
    "\n",
    "# If set, create a continuous variable that convolves a decaying exponential with stimulus\n",
    "# onset times\n",
    "if LEAKY_INTEGRATOR:\n",
    "\n",
    "    ## extract stimulus masks and indexes ##\n",
    "\n",
    "    print(f\"Creating continuous 'leaky integrator' variable for stimuli...\")\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        (_,stim_on_fr_list,stim_on_fr_list_half,_,stim_on_mask_list,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                        stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "        \n",
    "    ## create a new mask with stim_on indexes removed and first-post-stim indexes labelled '1' ## \n",
    "\n",
    "    num_stims = len(stim_on_fr_list)\n",
    "\n",
    "    # create a 'trigger' list of the first value post each stimulus\n",
    "    stim_trigger_fr_list = []\n",
    "    # for each stim\n",
    "    for i in range(num_stims):\n",
    "        stim_on_fr_list[i] = np.asarray(stim_on_fr_list[i])\n",
    "        stim_trigger_fr_list.append(stim_on_fr_list[i] + STIM_LENGTH_FRAMES)\n",
    "        assert(stim_trigger_fr_list[i][0] == stim_on_fr_list[i][0] + STIM_LENGTH_FRAMES)\n",
    "        \n",
    "\n",
    "    # create stimulus mask with trigger indexes labelled\n",
    "    trigger_mask_list = []\n",
    "    # for each stim\n",
    "    for i in range(num_stims):\n",
    "        this_stim_trigger_mask = np.zeros(stim_on_mask_list[i].shape)\n",
    "        this_stim_triggers = stim_trigger_fr_list[i]\n",
    "        this_stim_trigger_mask[this_stim_triggers] = 1\n",
    "        trigger_mask_list.append(this_stim_trigger_mask)\n",
    "\n",
    "    # flip the 1s and 0s of the stimulus on mask to use for removing indexes\n",
    "    # for each stim\n",
    "    for i in range(num_stims):\n",
    "        stim_on_mask_list[i] = (stim_on_mask_list[i]*-1) + 1\n",
    "        assert(np.sum(np.bitwise_and(stim_on_mask_list[i], (stim_on_mask_list[i]*-1) + 1)) == 0)\n",
    "\n",
    "    # combine all stim_on_mask arrays in the list, to remove any and all stimuli\n",
    "    i = 0 \n",
    "    combined_mask = stim_on_mask_list[i]\n",
    "    while i < num_stims - 1:\n",
    "        combined_mask = np.bitwise_and(combined_mask, stim_on_mask_list[i+1])\n",
    "        i += 1\n",
    "    all_stim_on_mask = combined_mask\n",
    "\n",
    "\n",
    "    # remove indexes that correspond to stimulus on times (of all stimuli)\n",
    "    removed_stim_trigger_mask_list = []\n",
    "    # for each stim\n",
    "    for i in range(num_stims):\n",
    "        this_stim_trigger_mask = trigger_mask_list[i]\n",
    "        #this_stim_on_mask = stim_on_mask_list[i]\n",
    "        this_stim_removed_stim_trigger_mask = this_stim_trigger_mask[all_stim_on_mask == 1]\n",
    "\n",
    "        # new array should have num_stimuli*stim_duration fewer indexes than the full mask\n",
    "        num_stims_total = np.sum([stim_on_fr_list[i].size for i in range(len(stim_on_fr_list))])\n",
    "        assert(this_stim_removed_stim_trigger_mask.size == this_stim_trigger_mask.size - STIM_LENGTH_FRAMES*num_stims_total)\n",
    "\n",
    "        removed_stim_trigger_mask_list.append(this_stim_removed_stim_trigger_mask)\n",
    "\n",
    "    # finally, find all the indexes that represent the post-stimulus frame\n",
    "    post_stim_idxs_list = []\n",
    "    # for each stim\n",
    "    for i in range(num_stims):\n",
    "        this_stim_removed_stim_trigger_mask = removed_stim_trigger_mask_list[i]\n",
    "        post_stim_idxs = np.asarray(np.where(this_stim_removed_stim_trigger_mask == 1)[0])\n",
    "        post_stim_idxs_list.append(post_stim_idxs)\n",
    "        \n",
    "    # # use stimulus frame numbers to create a boolean array\n",
    "    # stim_onset_mask = []\n",
    "    # # loop through stim types\n",
    "    # for i in range(len(stim_on_fr_list)):\n",
    "    #     this_stim_fr = stim_on_fr_list[i]\n",
    "    #     this_stim_onset_mask = np.zeros(neural.shape[0])\n",
    "    #     this_stim_onset_mask[this_stim_fr] = 1\n",
    "    #     stim_onset_mask.append(this_stim_onset_mask.astype(int))\n",
    "\n",
    "        \n",
    "    ## create decaying exponential kernel to convolve with ##\n",
    "\n",
    "    def exp_decay(x, param=0.1, n_zero=100):\n",
    "        \n",
    "        return n_zero*np.exp(-x*param)\n",
    "\n",
    "\n",
    "    # For mode='valid', any points where the data and kernel do completely overlap are\n",
    "    # truncated. This means the first kernel_size datapoints, but then up to the max size of the\n",
    "    # dataset. For this case I should take 'full', remove kernel_size points from the end, and then\n",
    "    # remove points up until the first stimulus presentation\n",
    "    kernel_size = 20000\n",
    "    x_vals = np.arange(kernel_size)\n",
    "    decay_constant = (1e-2)/2\n",
    "    kernel = exp_decay(x_vals, param=decay_constant, n_zero=100)\n",
    "\n",
    "    ## do convolution ##\n",
    "\n",
    "    convolved_stim_onset_fr = []\n",
    "    # loop over stimuli\n",
    "    for i in range(len(removed_stim_trigger_mask_list)):\n",
    "        this_convolved_stim = np.convolve(removed_stim_trigger_mask_list[i], kernel, mode='full')\n",
    "        convolved_stim_onset_fr.append(this_convolved_stim)\n",
    "    print(\"Convolution complete.\")\n",
    "\n",
    "    ## process convolved and neural data ##\n",
    "\n",
    "    # neural\n",
    "    # again, remove stim_on indexes\n",
    "    neural_leaky_integrator = neural[all_stim_on_mask == 1]\n",
    "\n",
    "    # separate into test and train\n",
    "    split_idx = int(TRAINING_TEST_SPLIT*neural_leaky_integrator.shape[0])\n",
    "    neural_test = neural_leaky_integrator[split_idx:]\n",
    "    neural_train = neural_leaky_integrator[:split_idx]\n",
    "    \n",
    "    # convolved\n",
    "    # loop over stimuli\n",
    "    for i in range(len(removed_stim_trigger_mask_list)):\n",
    "        # truncate any points added on past the end of the original signal\n",
    "        convolved_stim_onset_fr[i] = convolved_stim_onset_fr[i][:neural_leaky_integrator.shape[0]]\n",
    "\n",
    "    # arrange continuous variable as expected input to CEBRA\n",
    "    cont_var = np.vstack(convolved_stim_onset_fr).T\n",
    "\n",
    "    # separate into test and train\n",
    "    cont_train = cont_var[:split_idx, :]\n",
    "    cont_test = cont_var[split_idx:, :]\n",
    "\n",
    "\n",
    "\n",
    "    # # assert dataset sizes\n",
    "    # # neural_test should have the correct training set size removed and the time until\n",
    "    # # the end of the first round of stimulus presentations removed.\n",
    "    assert neural_test.shape[0] == neural_leaky_integrator.shape[0] - split_idx\n",
    "\n",
    "    print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, find all the indexes that represent the post-stimulus frame\n",
    "post_stim_idxs_list = []\n",
    "# for each stim\n",
    "for i in range(num_stims):\n",
    "    this_stim_removed_stim_trigger_mask = removed_stim_trigger_mask_list[i]\n",
    "    post_stim_idxs = np.asarray(np.where(this_stim_removed_stim_trigger_mask == 1)[0])\n",
    "    post_stim_idxs_list.append(post_stim_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,   -44,   -88,  -132,  -176,  -220,  -264,  -308,  -352,\n",
       "        -396,  -440,  -484,  -528,  -572,  -616,  -660,  -704,  -748,\n",
       "        -792,  -836,  -880,  -924,  -968, -1012, -1056, -1100, -1144,\n",
       "       -1188, -1232, -1276, -1320])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be num_stim_types * STIM_LENGTH_FRAMES each time, because one of each stimulus type is cut every stim cycle\n",
    "post_stim_idxs_list[0] - stim_on_fr_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final leaky integrator version checks #\n",
    "\n",
    "# leaky integrator indexes should be the size of neural but with num_stims*stim_duration frames missing\n",
    "diff = neural.shape[0] - all_stim_on_mask[all_stim_on_mask == 1].shape[0]\n",
    "diff2 = STIM_LENGTH_FRAMES*stim_on_fr_list[0].size + STIM_LENGTH_FRAMES*stim_on_fr_list[1].size\n",
    "assert(diff == diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zero = 100\n",
    "param = 0.25\n",
    "x = np.arange(25)\n",
    "\n",
    "a = n_zero*np.exp(-x*param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous data only.\n"
     ]
    }
   ],
   "source": [
    "### Create peri-stimulus \"trials\" ### \n",
    "\n",
    "if TRIAL_TIME:\n",
    "\n",
    "    ## params ##\n",
    "    pad = 4                      # time in s to pad either side of the stimulus in trial time\n",
    "    dur = STIM_LENGTH_FRAMES     # length of stimulus in frames\n",
    "    fr = FRAMERATE               # global recording framerate \n",
    "\n",
    "    # paths\n",
    "    # separate stim on mask and neural data for this trial structure\n",
    "    filename_neural_subset_trial_time = f'{filename[-9:]}_{SIGNAL_TYPE}_subset_trial_time.npz'\n",
    "    filename_stim_on_mask_trial_time = f'{filename[-9:]}_stim_on_mask_trial_time.npz'\n",
    "\n",
    "\n",
    "    ## access data ## \n",
    "\n",
    "    # create stimulus presentation masks, and find stimulus onset and end times\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "        (stim_pres_idx_list, \n",
    "            stim_on_fr_list,\n",
    "            stim_on_fr_list_half,\n",
    "            stim_end_fr_list, \n",
    "            stim_on_mask_list, \n",
    "            stim_on_mask_list_half,\n",
    "            stim_dur_list)          = create_stimulus_presentation_masks(f, neural, stims, stim_types,\n",
    "                                                                         stim_length_frames, timesteps=None)\n",
    "\n",
    "\n",
    "    # for each stimulus, combine stim on and off times into the same array\n",
    "    # (to get a general array of stimulus timings)\n",
    "    spot_stim_on_frames = np.sort(np.concatenate([stim_on_fr_list[0], stim_on_fr_list[1]]))\n",
    "    spot_stim_end_frames = np.sort(np.concatenate([stim_end_fr_list[0], stim_end_fr_list[1]]))\n",
    "\n",
    "    # separate out stimulus ON masks into left spot and right \n",
    "    left_spot, right_spot = np.copy(stim_on_mask_list[0]), np.copy(stim_on_mask_list[1])\n",
    "    left_spot_half, right_spot_half = np.copy(stim_on_mask_list_half[0]), np.copy(stim_on_mask_list_half[1])\n",
    "\n",
    "    ## crop data into peri-trial time only ##\n",
    "\n",
    "    # initialise numpy arrays (broadcast over rows)\n",
    "    neural_list, discrete_list_left, discrete_list_right = np.zeros((3, spot_stim_on_frames.size), dtype='object')\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        cont_list_left, cont_list_right = np.zeros((2, spot_stim_on_frames.size), dtype='object')\n",
    "\n",
    "    # for every stimulus presentation, crop neural and stimulus data to a window around the presentation\n",
    "    # defined by stimulation duration, data framerate, and amount of padding (pad)\n",
    "    for i in range(len(spot_stim_on_frames)):\n",
    "        pres = spot_stim_on_frames[i]\n",
    "\n",
    "        # neural\n",
    "        neural_list[i] = neural[pres - pad*fr: pres + dur + pad*fr, :]\n",
    "        \n",
    "        # left spot\n",
    "        # if taking second half of stims, crop the relevant array\n",
    "        if HALF_STIM_MASK:\n",
    "            discrete_list_left[i] = left_spot_half[pres - pad*fr: pres + dur + pad*fr]\n",
    "        else:\n",
    "            discrete_list_left[i] = left_spot[pres - pad*fr: pres + dur + pad*fr]\n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            cont_list_left[i] = left_spot_cont[pres -pad*fr: pres + dur + pad*fr]\n",
    "        \n",
    "        # right spot\n",
    "        # if taking second half of stims, crop the relevant array\n",
    "        if HALF_STIM_MASK:\n",
    "            discrete_list_right[i] = right_spot_half[pres - pad*fr: pres + dur + pad*fr]\n",
    "        else:   \n",
    "            discrete_list_right[i] = right_spot[pres - pad*fr: pres + dur + pad*fr]\n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            cont_list_right[i] = right_spot_cont[pres -pad*fr: pres + dur + pad*fr]\n",
    "\n",
    "    ## format data ##\n",
    "\n",
    "    # format the discrete variable by combining stimuli\n",
    "    # and set right spot value as 2 to differentiate from left spot\n",
    "    discrete_list = np.zeros(discrete_list_left.size, dtype='object')\n",
    "    for trial in range(discrete_list_left.size):\n",
    "        right_spot_trial = np.multiply(discrete_list_right[trial], 2)\n",
    "        discrete_list[trial] = np.add(discrete_list_left[trial], right_spot_trial)\n",
    "\n",
    "    ## save data ##\n",
    "\n",
    "    print(\"Saving trial-time data...\")\n",
    "    np.savez(f\"{data_folder}{filename_neural_subset_trial_time}\", neural_trial=neural_list)\n",
    "    np.savez(f\"{data_folder}{filename_stim_on_mask_trial_time}\", stim_on_mask=discrete_list)\n",
    "    print(f\"All datasets saved.\")\n",
    "\n",
    "    ## training-test split ##\n",
    "\n",
    "    # find the number of trials belonging to test and train sets\n",
    "    training_test_split = TRAINING_TEST_SPLIT\n",
    "    num_trials = int(len(neural_list))\n",
    "    num_train_trials = int(np.floor(training_test_split*num_trials))\n",
    "\n",
    "    # if only taking the last trials as test data\n",
    "    if not RANDOMISED_TEST:\n",
    "\n",
    "        # index lists with training and test indices\n",
    "        train_trials_idx = np.arange(num_train_trials)\n",
    "        test_trials_idx = np.arange(num_train_trials+1, num_trials)\n",
    "        neural_train, neural_test = np.concatenate(neural_list[train_trials_idx]), np.concatenate(neural_list[test_trials_idx])\n",
    "        discrete_train, discrete_test = np.concatenate(discrete_list[train_trials_idx]), np.concatenate(discrete_list[test_trials_idx])\n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            cont_left_test, cont_right_test = np.concatenate(cont_list_left[test_trials_idx]), \\\n",
    "                                                np.concatenate(cont_list_right[test_trials_idx])\n",
    "            cont_left_train, cont_right_train = np.concatenate(cont_list_left[train_trials_idx]), \\\n",
    "                                                np.concatenate(cont_list_right[train_trials_idx])\n",
    "\n",
    "    # if taking a random subset of trials as test data\n",
    "    elif RANDOMISED_TEST:\n",
    "\n",
    "        # select random trials equal to the test set size\n",
    "        num_test_trials = num_trials - num_train_trials\n",
    "        test_trials_idx = np.random.choice(np.arange(num_trials), size=num_test_trials)\n",
    "        # subtract the set of test trials from all trials to find the set of train trials\n",
    "        train_trials_idx =  list(set(np.arange(num_trials)) - set(test_trials_idx))\n",
    "\n",
    "        # index the lists of trials to create test and train data\n",
    "        (neural_test, \n",
    "            discrete_test) = (np.concatenate(neural_list[test_trials_idx]),\n",
    "                            np.concatenate(discrete_list[test_trials_idx]))\n",
    "        (neural_train, \n",
    "            discrete_train) = (np.concatenate(neural_list[train_trials_idx]),\n",
    "                            np.concatenate(discrete_list[train_trials_idx]))            \n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            (cont_left_test,\n",
    "                cont_right_test) = (np.concatenate(cont_list_left[test_trials_idx]),\n",
    "                                    np.concatenate(cont_list_right[test_trials_idx]))\n",
    "            (cont_left_train,\n",
    "                cont_right_train) = (np.concatenate(cont_list_left[train_trials_idx]),\n",
    "                                    np.concatenate(cont_list_right[train_trials_idx]))\n",
    "    # end if/elif \n",
    "\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        # if using continuous variable, format it as expected by CEBRA\n",
    "        cont_test = np.vstack((cont_left_test, cont_right_test)).T\n",
    "        cont_train = np.vstack((cont_left_train, cont_right_train)).T\n",
    "\n",
    "else:\n",
    "    print(\"Continuous data only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRIAL_TIME:\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    # create a concatenated version for plotting\n",
    "    neural_trial = np.concatenate(neural_list)\n",
    "    discrete_trial = np.concatenate(discrete_list)\n",
    "    plt.plot(np.mean(neural_trial[:], axis=1), label='mean neural activity')\n",
    "    plt.plot(discrete_trial[:]/50, alpha=0.5, label='spot stimulus')\n",
    "    plt.legend(loc='upper right', fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRIAL_TIME:\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "    ax1.plot(np.mean(neural_list[45], axis=1))\n",
    "    ax2.plot(neural_list[45])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39melif\u001b[39;00m LOAD_MODEL:  \u001b[39m# do no fitting on a loaded model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m## load ##\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading model...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     cebra_stim_on_model \u001b[39m=\u001b[39m cebra\u001b[39m.\u001b[39;49mCEBRA\u001b[39m.\u001b[39;49mload(full_model_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel loaded.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mif\u001b[39;00m SAVE_MODEL:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tomh/Documents/projects/czebra/test_notebooks/231030/decode_stimulus_laterality_tectal_leaky-integrator_NEWEST.ipynb#X31sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         \u001b[39m## save ##\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/cebra/integrations/sklearn/cebra.py:1254\u001b[0m, in \u001b[0;36mCEBRA.load\u001b[0;34m(cls, filename, backend, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1253\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported backend: \u001b[39m\u001b[39m{\u001b[39;00mbackend\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1254\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mcls\u001b[39m):\n\u001b[1;32m   1256\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mModel loaded from file is not compatible with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1257\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mthe current CEBRA version.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/envs/czebra/lib/python3.8/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "### load, fit, and save model ###\n",
    "# if LOAD_MODEL == True, load the specified model. Otherwise, fit the model\n",
    "# and additionally save it if SAVE_MODEL == True\n",
    "\n",
    "modelpath_root = MODELPATH_ROOT\n",
    "modelpath_date = MODELPATH_DATE\n",
    "\n",
    "# create model name and full model path\n",
    "# model_name = 'f1_221103_spot-label_10000rois_2000iters_128hidden_1e-4L2'\n",
    "model_name = f\"{filename}_{LABEL}{'-label' if parameters['conditional'] == 'time_delta' else 'time'}_\" + \\\n",
    "    f\"{EXTRA_LABEL+'_' if EXTRA_LABEL else ''}\" + \\\n",
    "    f\"{ROIS}rois_{ITERS}iters_{parameters['num_hidden_units']}hidden_{parameters['weight_decay']}L2_\" + \\\n",
    "    f\"{TRAINING_TEST_SPLIT}train\"\n",
    "full_model_path = f\"{modelpath_root}{modelpath_date}{model_name}/{model_name}_{MODEL_VER}.pt\"\n",
    "model_dir = f\"{modelpath_root}{modelpath_date}{model_name}\"\n",
    "\n",
    "\n",
    "# if partial fitting the model and saving it, first load the model and continue with fitting, then save\n",
    "if SAVE_MODEL and PARTIAL_FIT:\n",
    "    \n",
    "    ## load ##\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    # Try to load the partial fit model. If it doesn't exist yet, skip this\n",
    "    # step and go straight to training\n",
    "    try:\n",
    "        cebra_stim_on_model = cebra.CEBRA.load(full_model_path)\n",
    "        print(\"Model loaded.\")\n",
    "    except:\n",
    "        print(\"Model not loaded. No model found; starting fresh training.\")\n",
    "\n",
    "\n",
    "    ## fit ## \n",
    "\n",
    "    print(\"Fitting model...\")\n",
    "    print(cebra_stim_on_model)\n",
    "\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        cebra_stim_on_model.partial_fit(neural_train, cont_train, discrete_train)\n",
    "        print(\"discrete and continuous stimulus model fit.\")\n",
    "\n",
    "    elif TIME_SINCE_LAST:\n",
    "        cebra_stim_on_model.partial_fit(neural_train, cont_train)\n",
    "        print(\"Time-since-last model fit.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Default training\")\n",
    "        cebra_stim_on_model.partial_fit(neural_train, discrete_train)\n",
    "        print(\"Discrete stimulus model fit.\")\n",
    "    \n",
    "elif LOAD_MODEL:  # do no fitting on a loaded model\n",
    "\n",
    "    ## load ##\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    cebra_stim_on_model = cebra.CEBRA.load(full_model_path)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "\n",
    "        ## save ##\n",
    "        print(\"Saving model...\")\n",
    "        cebra_stim_on_model.save(full_model_path)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "\n",
    "else:   # fit if not loading, or partial fitting\n",
    "\n",
    "    ## fit ## \n",
    "\n",
    "    print(\"Fitting model...\")\n",
    "    print(cebra_stim_on_model)\n",
    "\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        cebra_stim_on_model.fit(neural_train, cont_train, discrete_train)\n",
    "        print(\"discrete and continuous stimulus model fit.\")\n",
    "\n",
    "    elif TIME_SINCE_LAST:\n",
    "        cebra_stim_on_model.fit(neural_train, cont_train)\n",
    "        print(\"Time-since-last model fit.\")\n",
    "    \n",
    "    elif LEAKY_INTEGRATOR:\n",
    "        cebra_stim_on_model.fit(neural_train, cont_train)\n",
    "        print(\"Leaky integrator model fit.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Default training\")\n",
    "        cebra_stim_on_model.fit(neural_train, discrete_train)\n",
    "        print(\"Discrete stimulus model fit.\")\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "        \n",
    "        ## save ## \n",
    "\n",
    "        print(\"Saving model...\")\n",
    "        # create the directory if it does not already exist (and continue if it does)\n",
    "        Path(f\"{model_dir}\").mkdir(parents=True, exist_ok=True)\n",
    "        cebra_stim_on_model.save(full_model_path)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure to show the kernel used ###\n",
    "x_test = np.arange(1000)\n",
    "y_test = exp_decay(x_test, param=decay_constant, n_zero=1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test)\n",
    "\n",
    "try:\n",
    "    os.mkdir(f\"{model_dir}/v{MODEL_VER}/\")\n",
    "except:\n",
    "    pass\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}convolution-kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure to show mean neural activity against both the convolved\n",
    "  # continuous signal and the actual stimulus onset frame ###\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Creating continuous 'leaky integrator' variable for stimuli...\")\n",
    "with h5py.File(filepath, 'r') as f:\n",
    "    (_,temp_on_fr_list,temp_on_fr_list_half,_,_,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                    stim_types,\n",
    "                                                            STIM_LENGTH_FRAMES, timesteps)\n",
    "\n",
    "min_frame = 1500\n",
    "max_frame = 5500\n",
    "left_stim_onsets = post_stim_idxs_list[0][post_stim_idxs_list[0] < max_frame]\n",
    "left_stim_onsets = left_stim_onsets[left_stim_onsets > min_frame]\n",
    "right_stim_onsets = post_stim_idxs_list[1][post_stim_idxs_list[1] < max_frame]\n",
    "right_stim_onsets = right_stim_onsets[right_stim_onsets > min_frame]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(convolved_stim_onset_fr[0][min_frame:max_frame], label='left')\n",
    "ax.plot(convolved_stim_onset_fr[1][min_frame:max_frame], label='right')\n",
    "ax.scatter(left_stim_onsets-min_frame, np.ones(left_stim_onsets.shape)*100, color='b')\n",
    "ax.scatter(right_stim_onsets-min_frame, np.ones(right_stim_onsets.shape)*100, color='orange')\n",
    "ax.plot(np.mean(neural_leaky_integrator[min_frame:max_frame], axis=1)*20*100, alpha=0.5, label='neural mean')\n",
    "plt.show()\n",
    "plt.legend(fontsize='x-small')\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}convolved-signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_on_fr_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frame = 0\n",
    "max_frame = 20000\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.mean(neural_leaky_integrator[min_frame:max_frame], axis=1)*20*100, alpha=0.5, label=' cut neural mean')\n",
    "ax.plot(np.mean(neural[min_frame:max_frame], axis=1)*20*100, alpha=0.5, label='neural mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate/plot embeddings ###\n",
    "\n",
    "laterality = 0\n",
    "\n",
    "# # CEBRA's plot_embedding function is bugged\n",
    "fig = plt.figure(figsize=(6,12))\n",
    "ax1 = plt.subplot(211, projection='3d')\n",
    "ax2 = plt.subplot(212, projection='3d')\n",
    "ax1.view_init(elev=20., azim=45)\n",
    "ax2.view_init(elev=20., azim=45)\n",
    "\n",
    "# trained embedding\n",
    "embedding_train = cebra_stim_on_model.transform(neural_train)\n",
    "cebra.plot_embedding(embedding=embedding_train, embedding_labels=cont_train[:,laterality], ax=ax1, cmap='cool', title=\"Training Embedding\")\n",
    "\n",
    "# test embedding\n",
    "embedding_test = cebra_stim_on_model.transform(neural_test)\n",
    "cebra.plot_embedding(embedding=embedding_test, embedding_labels=cont_test[:,laterality], ax=ax2, cmap='cool', title=\"Test embedding\")\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot loss and temperature ###\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "cebra.plot_loss(cebra_stim_on_model, ax=ax1)\n",
    "cebra.plot_temperature(cebra_stim_on_model, ax=ax2)\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}loss-and-temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decode ###\n",
    "\n",
    "# 0 = left, 1 = right\n",
    "laterality = 0\n",
    "\n",
    "## set what should be predicted ##\n",
    "# this should be function input\n",
    "if TIME_SINCE_LAST or LEAKY_INTEGRATOR:\n",
    "    predicted_var = cont_train\n",
    "else:\n",
    "    predicted_var = discrete_train\n",
    "\n",
    "# if variable to be predicted is multidimensional, take the correct laterality for\n",
    "# decoding\n",
    "if predicted_var.ndim > 1:\n",
    "    predicted_var_subset = predicted_var[:,laterality]\n",
    "else:\n",
    "    predicted_var_subset = predicted_var\n",
    "\n",
    "decoder = cebra.KNNDecoder()\n",
    "\n",
    "decoder.fit(embedding_train, predicted_var_subset)\n",
    "\n",
    "# decide whether to predict on the same embedding as trained on, or on the test embedding\n",
    "if PREDICT_ON_TRAIN:\n",
    "    # changed to predict on the training embedding\n",
    "    predictions_left = decoder.predict(embedding_train)\n",
    "    # also change what is considered the \"test\" set. In this case it will be the training set\n",
    "    cont_test = cont_train\n",
    "else:\n",
    "    predictions_left = decoder.predict(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualise ###\n",
    "\n",
    "# left = 0, right = 1\n",
    "laterality = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.plot(cont_test[:,laterality], label='ground truth')\n",
    "# ax1.plot(predictions, label='prediction')\n",
    "ax1.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax2.plot(predictions_left, label='prediction')\n",
    "ax2.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax3.plot(cont_test[:,laterality], label='ground truth')\n",
    "ax3.plot(predictions_left, label='predictions', alpha=0.6)\n",
    "ax3.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}predictions_left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decode ## \n",
    "## Copied for the right stimulus ## \n",
    "\n",
    "# 0 = left, 1 = right\n",
    "laterality = 1\n",
    "\n",
    "## set what should be predicted ##\n",
    "# this should be function input\n",
    "if TIME_SINCE_LAST or LEAKY_INTEGRATOR:\n",
    "    predicted_var = cont_train\n",
    "else:\n",
    "    predicted_var = discrete_train\n",
    "\n",
    "# if variable to be predicted is multidimensional, take the correct laterality for\n",
    "# decoding\n",
    "if predicted_var.ndim > 1:\n",
    "    predicted_var_subset = predicted_var[:,laterality]\n",
    "else:\n",
    "    predicted_var_subset = predicted_var\n",
    "\n",
    "decoder = cebra.KNNDecoder()\n",
    "\n",
    "decoder.fit(embedding_train, predicted_var_subset)\n",
    "\n",
    "# decide whether to predict on the same embedding as trained on, or on the test embedding\n",
    "if PREDICT_ON_TRAIN:\n",
    "    # changed to predict on the training embedding\n",
    "    predictions_right = decoder.predict(embedding_train)\n",
    "    # also change what is considered the \"test\" set. In this case it will be the training set\n",
    "    cont_test = cont_train\n",
    "else:\n",
    "    predictions_right = decoder.predict(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualise ###\n",
    "\n",
    "# left = 0, right = 1\n",
    "laterality = 1\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.plot(cont_test[:,laterality], label='ground truth')\n",
    "# ax1.plot(predictions, label='prediction')\n",
    "ax1.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax2.plot(predictions_right, label='prediction')\n",
    "ax2.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax3.plot(cont_test[:,laterality], label='ground truth')\n",
    "ax3.plot(predictions_right, label='predictions', alpha=0.6)\n",
    "ax3.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}predictions_right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_left[:10], predictions_right[:10], predictions_left[:10]+predictions_right[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### analyse predictions ###\n",
    "## Now done for both left and right data concatenated ## \n",
    "\n",
    "# This should really be: 'if predicted variable takes continuous values and not discrete'\n",
    "if LEAKY_INTEGRATOR:\n",
    "    \n",
    "    predictions = np.concatenate((predictions_left,predictions_right), axis=0)\n",
    "    if PREDICT_ON_TRAIN:\n",
    "        left_right_cont = np.concatenate((cont_train[:,0], cont_train[:,1]), axis=0)\n",
    "    else:\n",
    "        left_right_cont = np.concatenate((cont_test[:,0], cont_test[:,1]), axis=0)\n",
    "\n",
    "\n",
    "    ## find smoothed predictions ##\n",
    "    # use a rolling average filter\n",
    "    kernel_size = 50\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "    data = np.copy(predictions)\n",
    "\n",
    "    # run filter\n",
    "    smoothed_data = np.convolve(data, kernel, mode='full')\n",
    "\n",
    "    # plot smoothed data \n",
    "    fig, axs = plt.subplots(2,1, figsize=(10,8))\n",
    "    axs[0].plot(data, label='predictions', alpha=0.6)\n",
    "    axs[0].plot(smoothed_data, label='smoothed')\n",
    "    axs[0].legend(loc='upper right', fontsize='small')\n",
    "\n",
    "\n",
    "    axs[1].plot(data, label='predictions', alpha=0.6)\n",
    "    axs[1].plot(smoothed_data, label='smoothed')\n",
    "    axs[1].plot(left_right_cont, label='ground truth')\n",
    "    # axs[1].plot(cont_test[:,1], label='ground truth right')\n",
    "    axs[1].legend(loc='upper right', fontsize='small')\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        # save this figure to the model folder\n",
    "        plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}\" + \\\n",
    "                    f\"smoothed_predictions_left-and-right\")\n",
    "\n",
    "    ## find stimulus-triggered average predictions ## \n",
    "\n",
    "    n_seconds = 40\n",
    "    fr = FRAMERATE\n",
    "    num_frames_post = n_seconds*fr\n",
    "    num_frames_pre = int(n_seconds*fr*0.2)\n",
    "    data = np.copy(predictions)\n",
    "\n",
    "    # find stim on times\n",
    "    ## REPEAT FOR BOTH LEFT AND RIGHT ##\n",
    "    ## LEFT ##  \n",
    "    laterality = 0 \n",
    "    # Remember, here we are using indexes that take into account the cut stimulus on time\n",
    "    chosen_stim_on_frames_left= post_stim_idxs_list[laterality]\n",
    "    if not PREDICT_ON_TRAIN: # use test set where relevant\n",
    "        chosen_stim_on_frames_left = chosen_stim_on_frames_left[chosen_stim_on_frames_left > split_idx]\n",
    "    else:\n",
    "        chosen_stim_on_frames_left = chosen_stim_on_frames_left[chosen_stim_on_frames_left < split_idx]\n",
    "                                                      \n",
    "\n",
    "    # create an array of n seconds around the stimulus\n",
    "    peri_stim_left = np.zeros((chosen_stim_on_frames_left.size, num_frames_post+num_frames_pre))\n",
    "    for i in range(chosen_stim_on_frames_left.size):\n",
    "        if PREDICT_ON_TRAIN:\n",
    "            stim_idx = chosen_stim_on_frames_left[i]\n",
    "        else:\n",
    "            stim_idx = chosen_stim_on_frames_left[i] - split_idx     # test-set-centric index\n",
    "        peri_stim_left[i] = data[stim_idx-num_frames_pre : stim_idx+num_frames_post]\n",
    "\n",
    "    # find stim on times\n",
    "    ## REPEAT FOR BOTH LEFT AND RIGHT ## \n",
    "    ## RIGHT ## \n",
    "    laterality = 1 \n",
    "    # Remember, here we are using indexes that take into account the cut stimulus on time\n",
    "    ## ACCOUNT FOR IDX CHANGES FROM CONCATENATIONS\n",
    "    chosen_stim_on_frames_right = post_stim_idxs_list[laterality]\n",
    "    if not PREDICT_ON_TRAIN: # use test set where relevant\n",
    "        chosen_stim_on_frames_right = chosen_stim_on_frames_right[chosen_stim_on_frames_right > split_idx]\n",
    "    else:\n",
    "        chosen_stim_on_frames_right = chosen_stim_on_frames_right[chosen_stim_on_frames_right < split_idx]\n",
    "    chosen_stim_on_frames_right = chosen_stim_on_frames_right + cont_test.shape[0]\n",
    "\n",
    "    # create an array of n seconds around the stimulus\n",
    "    peri_stim_right = np.zeros((chosen_stim_on_frames_right.size, num_frames_post+num_frames_pre))\n",
    "    for i in range(chosen_stim_on_frames_right.size):\n",
    "        if PREDICT_ON_TRAIN:\n",
    "            stim_idx = chosen_stim_on_frames_right[i]\n",
    "        else:\n",
    "            stim_idx = chosen_stim_on_frames_right[i] - split_idx     # test-set-centric index\n",
    "        peri_stim_right[i] = data[stim_idx-num_frames_pre : stim_idx+num_frames_post]\n",
    "\n",
    "    ## NOW CREATE A CONCATENATED ARRAY OF PERI-STIMS ##\n",
    "\n",
    "    peri_stim = np.vstack((peri_stim_left, peri_stim_right))\n",
    "    # produce a mean average of this array\n",
    "    peri_stim_avg = np.mean(peri_stim, axis=0)\n",
    "\n",
    "    # plot\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,4))\n",
    "    axs[0].plot(peri_stim_avg)\n",
    "    axs[0].vlines(num_frames_pre, 0,120, color='black', linestyle='dashed')\n",
    "\n",
    "    # also plot a smoothed version\n",
    "    kernel_size = 10\n",
    "    kernel = np.ones(kernel_size)/kernel_size\n",
    "    smoothed_peri_stim_avg = np.convolve(peri_stim_avg, kernel)\n",
    "    axs[1].plot(smoothed_peri_stim_avg)\n",
    "    axs[1].vlines(num_frames_pre, 0,120, color='black', linestyle='dashed')\n",
    "\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        # save this figure to the model folder\n",
    "        plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}\" + \\\n",
    "                    f\"stim-triggered-mean_long-form_left-and-right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### quantify continuous predictions (MSE) ###\n",
    "\n",
    "\n",
    "# This should really be: 'if predicted variable takes continuous values and not discrete'\n",
    "if LEAKY_INTEGRATOR:\n",
    "    laterality = 0 \n",
    "    kernel_size=50\n",
    "\n",
    "    predictions_smoothed = smoothed_data[:-(kernel_size - 1)]  # leave off extra datapoints created by kernel\n",
    "    left_right_cont_test = np.concatenate((cont_test[:,0], cont_test[:,1]), axis=0)\n",
    "    ground_truth = left_right_cont_test\n",
    "\n",
    "    # root mean squared error between predictions and ground truth\n",
    "    def RMSE(predictions, ground_truth):\n",
    "        diff = predictions - ground_truth\n",
    "        diff_sq = np.square(diff)\n",
    "        MSE = np.mean(diff_sq)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "\n",
    "        return RMSE\n",
    "\n",
    "    # run eval\n",
    "    rmse = RMSE(predictions_smoothed, ground_truth)\n",
    "\n",
    "    # append output to file\n",
    "    eval_file = f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}\" + \\\n",
    "                            f\"RMSE.txt\"\n",
    "    with open(eval_file, 'a+') as f:\n",
    "        f.write(f\"{rmse}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
