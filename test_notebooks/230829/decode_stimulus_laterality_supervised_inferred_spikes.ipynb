{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SPIM data CEBRA model\n",
    "\n",
    "- Use CEBRA label contrastive learning on neural data from one fish\n",
    "    - design model\n",
    "    - convert SPIM data to usable format \n",
    "    - load data\n",
    "    - fit with label\n",
    "    - plot embeddings\n",
    "    - try to predict stimulus presence\n",
    "    - try to decode stimulus type (left/right spots)\n",
    "        - create a discrete variable that labels the post-stimulus frames for right and left spots\n",
    "        - This should inform the decoder to separate embedding states (which should vary between left and right spots)<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cebra\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define globals ### \n",
    "\n",
    "# list of all data files\n",
    "dat_files = ['/media/storage/DATA/lfads_export/f1_221027.h5',\n",
    "             '/media/storage/DATA/lfads_export/f1_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f2_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f3_221103.h5']\n",
    "\n",
    "global LABEL                   # brief description of model training label\n",
    "global EXTRA_LABEL             # optional extra information to include in model name\n",
    "global FILEPATH                # path to data file\n",
    "global DATA_PATH               # path to experimental data\n",
    "global MODELPATH_ROOT          # folder path for models folder\n",
    "global MODELPATH_DATE          # specific path within models folder\n",
    "global MODEL_VER               # append version to model name for when saving multiple repeats\n",
    "global TIMESTEPS               # no. of neural timepoints to use\n",
    "global ROIS                    # no. of ROIs to use \n",
    "global ITERS                   # model training iterations\n",
    "global LOAD_DATA               # attempt to load pre-saved .npz data files\n",
    "global SAVE_DATA               # save new .npz files when created\n",
    "global LOAD_MODEL              # attempt to load model\n",
    "global SAVE_MODEL              # save model to models folder\n",
    "global STIM_TYPES              # dictionary of stim types to stim numbers\n",
    "global STIMS                   # relevant stims for model\n",
    "global STIM_MASKS              # remove? \n",
    "global TRAINING_TEST_SPLIT     # split fraction for embedding model\n",
    "global STIM_LENGTH_FRAMES      # the length in frames of stimuli, assuming all stimuli have equal duration\n",
    "global HALF_STIM_MASK          # only use the second half of stimuli for training the model\n",
    "\n",
    "LABEL = 'spot'                                                          # set as '' for time-contrastive\n",
    "EXTRA_LABEL = 'second-half-stims'\n",
    "FILEPATH = dat_files[1]\n",
    "DATA_PATH = '/home/tomh/Documents/projects/czebra/test_notebooks/data/'\n",
    "MODELPATH_ROOT = '/home/tomh/Documents/projects/czebra/archived_models/'\n",
    "MODELPATH_DATE = '230822/'\n",
    "MODEL_VER = 8\n",
    "TIMESTEPS = None                                                        # set as None to use all timepoints\n",
    "ROIS = 10000\n",
    "ITERS = 2000\n",
    "LOAD_DATA = True\n",
    "SAVE_DATA = False\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "STIM_TYPES = {'left_spot':0, 'right_spot':1,  \\\n",
    "              'open_loop_grating':2, 'closed_loop_grating':3}\n",
    "STIMS = ['left_spot', 'right_spot']\n",
    "STIM_MASKS = [['left_spot', 'right_spot']]\n",
    "TRAINING_TEST_SPLIT = 0.8\n",
    "STIM_LENGTH_FRAMES = 22\n",
    "HALF_STIM_MASK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model ###\n",
    "\n",
    "parameters = {'conditional':'time_delta',\n",
    "              'max_iterations':ITERS,\n",
    "              'batch_size':1024,\n",
    "              'num_hidden_units':128,\n",
    "              'weight_decay':1e-03,\n",
    "              'rois':ROIS,\n",
    "             }\n",
    "\n",
    "cebra_stim_on_model = cebra.CEBRA(\n",
    "    model_architecture='offset10-model',\n",
    "    device='cuda_if_available',\n",
    "    conditional=parameters['conditional'],\n",
    "    temperature_mode='auto',\n",
    "    min_temperature=0.1,\n",
    "    time_offsets=10,\n",
    "    max_iterations=parameters['max_iterations'],\n",
    "    max_adapt_iterations=500,\n",
    "    batch_size=parameters['batch_size'],\n",
    "    learning_rate=1e-4,\n",
    "    output_dimension=3,\n",
    "    verbose=True,\n",
    "    num_hidden_units=parameters['num_hidden_units'],\n",
    "    hybrid=False,\n",
    "    optimizer_kwargs=(('betas', (0.9, 0.999)), ('eps', 1e-08), ('weight_decay', parameters['weight_decay']), ('amsgrad', False))\n",
    "    )\n",
    "print(cebra_stim_on_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data for a single fish ###\n",
    "# if LOAD == True, load pre-saved .npz file data. Otherwise,\n",
    "# create this data as specified below and save it to .npz\n",
    "\n",
    "##  params ##\n",
    "\n",
    "# variables\n",
    "stim_types = STIM_TYPES     # dict of all possible stims\n",
    "stims = STIMS               # stim types chosen for analysis\n",
    "timesteps = TIMESTEPS\n",
    "rois = ROIS\n",
    "stim_length_frames = STIM_LENGTH_FRAMES # used for selecting the second half of stimuli\n",
    "\n",
    "start, stop = 0, timesteps\n",
    "load_data = LOAD_DATA\n",
    "save_data = SAVE_DATA\n",
    "\n",
    "# paths\n",
    "filepath = FILEPATH\n",
    "filename = filepath.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = DATA_PATH\n",
    "data_folder_HDD = '/media/storage/DATA/tom/'\n",
    "filename_stim_pres_frames = f'{filename[-9:]}_stim_pres_frames.npz'\n",
    "filename_deconv = f'{filename[-9:]}_deconv.npz'\n",
    "filename_deconv_subset = f'{filename[-9:]}_deconv_subset.npz'\n",
    "\n",
    "# if not loading data, but not wanting to overwrite saved data, save as a temp file\n",
    "if not save_data and not load_data: \n",
    "    print(f\"Producing temp files...\")\n",
    "    filename_deconv = f'{filename[-9:]}_deconv_TEMPORARY_DELETE.npz'\n",
    "    filename_deconv_subset = f'{filename[-9:]}_deconv_subset_TEMPORARY_DELETE.npz'\n",
    "    filename_stim_pres_frames = f'{filename[-9:]}_stim_pres_frames_TEMPORARY_DELETE.npz'\n",
    "\n",
    "\n",
    "print(\"Accessing data...\")\n",
    "\n",
    "if load_data:\n",
    "    \n",
    "    ## load data ##\n",
    "    \n",
    "    # Attempt to load neural data from .npz, otherwise load from HDD .h5\n",
    "    # Load small datasets from .npz files\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # neural\n",
    "    try:\n",
    "        key = \"neural\"\n",
    "        ## loading full dataset from HDD\n",
    "        # neural = cebra.load_data(filepath, key='rois/deconv')\n",
    "        ## loading full dataset from SSD\n",
    "        # neural = cebra.load_data(f\"{data_folder}{filename_deconv}\", key=key)\n",
    "        # loading subset of dataset that was just created and saved\n",
    "        neural = cebra.load_data(f\"{data_folder}{filename_deconv_subset}\", key=key)\n",
    "        print(\"Neural data loaded\")\n",
    "\n",
    "    except:\n",
    "        print(f\"Could not neural data from file {data_folder}{filename_deconv_subset} under key {key}.\")\n",
    "        # try:\n",
    "        #     neural = cebra.load_data(filepath, key='rois/deconv')\n",
    "        #     print(\"Full neural data loaded from HDD file.\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(\"Could not load data.\")\n",
    "        #     print(e)\n",
    "    \n",
    "    # auxiliary variables\n",
    "    stim_on_frames = cebra.load_data(f'{data_folder}{filename_stim_pres_frames}', key=\"stim_on_frames\")\n",
    "    print(f\"Stimulus presentation frames loaded.\")\n",
    "    print(\"All data loaded.\")\n",
    "\n",
    "# end if\n",
    "\n",
    "else:\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "            ## neural ##\n",
    "\n",
    "            neural = f['rois']['deconv']\n",
    "            print(f\"Full neural dataset shape is: {neural.shape}\")\n",
    "\n",
    "            # subset neural by selecting first TIMESTEPS timesteps and random ROIS rois\n",
    "            neural_indexes = np.sort(\n",
    "                                np.random.choice(\n",
    "                                            np.arange(neural.shape[1]), size=rois, replace=False\n",
    "                                            )\n",
    "                                )\n",
    "            \n",
    "            if timesteps:\n",
    "                neural = np.array(neural[:timesteps, neural_indexes])\n",
    "            else:\n",
    "                neural = np.array(neural[:, neural_indexes])\n",
    "                timesteps = neural.shape[0]\n",
    "\n",
    "            print(f\"Truncated dataset size: {neural.shape}\")\n",
    "            assert(neural.shape == (timesteps, rois))\n",
    "\n",
    "            ## stimuli ##\n",
    "\n",
    "            # get stimulus presentations\n",
    "            stimuli = f['visuomotor']['presentations']\n",
    "            stim_type = stimuli['stim_type'][:timesteps].astype(int)\n",
    "            stim_on_fr = stimuli['onset_frame'][:timesteps].astype(int)\n",
    "            stim_end_fr = stimuli['offset_frame'][:timesteps].astype(int)\n",
    "\n",
    "            # initialise lists for the chosen stimuli\n",
    "            (stim_pres_idx_list, stim_on_fr_list,\n",
    "            stim_end_fr_list, stim_on_mask_list, stim_dur_list)  = [],[],[],[],[]\n",
    "\n",
    "\n",
    "            # loop through chosen stimuli and find boolean masks for their 'on' frames\n",
    "            for stim in stims:\n",
    "\n",
    "                # convert stim name to stim number\n",
    "                stim_num = stim_types[stim] \n",
    "                print(f'Attempting to parse stim: {stim}') \n",
    "\n",
    "                # find the presentation indexes for the specified stim type\n",
    "                # must account for data index starting at 1\n",
    "                this_stim_pres_indexes = np.where(np.isin(stim_type, stim_num + 1))[0]\n",
    "                stim_pres_idx_list.append(this_stim_pres_indexes)\n",
    "\n",
    "                # index stim onset frame numbers with the presentation indexes\n",
    "                this_stim_on_frames = stim_on_fr[this_stim_pres_indexes]\n",
    "                this_stim_on_frames = this_stim_on_frames[this_stim_on_frames < timesteps]\n",
    "\n",
    "                # If taking only the second half of stimulus frames, increment the start frame\n",
    "                # indexes by half the stimulus duration, to simulate the stimulus starting halfway\n",
    "                # later than it actually does\n",
    "                if HALF_STIM_MASK: \n",
    "                    print(\"Taking the second half of stimuli only.\")\n",
    "                    this_stim_on_frames = this_stim_on_frames + stim_length_frames/2\n",
    "                stim_on_fr_list.append(this_stim_on_frames)\n",
    "\n",
    "                # index stim end frame numbers with the presentation indexes\n",
    "                this_stim_end_frames = stim_end_fr[this_stim_pres_indexes]\n",
    "                this_stim_end_frames = this_stim_end_frames[this_stim_end_frames < timesteps]\n",
    "                stim_end_fr_list.append(this_stim_end_frames)\n",
    "\n",
    "                # create a boolean mask of stimulus presentation frames (1 == stimulus on, 0 == stimulus off)\n",
    "                this_stim_on_mask = np.zeros(neural.shape[0]).astype(int)\n",
    "                this_stim_on_mask[[this_stim_on_frames, this_stim_end_frames]] = 1\n",
    "                \n",
    "                # perform bitwise XOR operation on consecutive elements of stim_on_mask. This will convert all \n",
    "                # but stim_off frame to 1s. Combining with \"OR stim_on_mask\" will also include the stim_off frame\n",
    "                this_stim_on_mask = np.bitwise_xor.accumulate(this_stim_on_mask) | this_stim_on_mask\n",
    "                stim_on_mask_list.append(this_stim_on_mask)\n",
    "\n",
    "                # find duration (in frames) of each presentation of the stimulus\n",
    "                # recording rate is 5 Hz\n",
    "                stim_dur_list.append(this_stim_end_frames - this_stim_on_frames)\n",
    "\n",
    "                # assert shapes\n",
    "                print(f\"Label shape: {stim_on_mask_list[0].shape}\")\n",
    "                assert(stim_on_mask_list[0].size == neural.shape[0])\n",
    "\n",
    "                print(f'Stim type {stim} parsed successfully.')\n",
    "\n",
    "            ## save data ##\n",
    "            print(\"Saving data...\")\n",
    "            stim_on_mask_dataset = np.column_stack(stim_on_mask_list[:])\n",
    "\n",
    "            assert(stim_on_mask_dataset.shape == (neural.shape[0], len(stims)))\n",
    "            assert(neural.shape == (timesteps, rois))\n",
    "\n",
    "            np.savez(f'{data_folder}{filename_stim_pres_frames}', stim_on_frames=stim_on_mask_dataset)\n",
    "            print(f\"Stim presentation dataset saved.\")\n",
    "            np.savez(f\"{data_folder}{filename_deconv_subset}\", neural=neural)\n",
    "            print(f\"Neural dataset saved.\")\n",
    "            print(f\"All datasets saved.\")\n",
    "\n",
    "            ## load data ##\n",
    "            # Attempt to load neural data from .npz, otherwise load from HDD .h5\n",
    "            # Load small datasets from .npz files\n",
    "            print(\"Loading data...\")\n",
    "            \n",
    "            # neural\n",
    "            try:\n",
    "                key = \"neural\"\n",
    "                ## loading full dataset from HDD\n",
    "                # neural = cebra.load_data(filepath, key='rois/deconv')\n",
    "                ## loading full dataset from SSD\n",
    "                # neural = cebra.load_data(f\"{data_folder}{filename_deconv}\", key=key)\n",
    "                # loading subset of dataset that was just created and saved\n",
    "                neural = cebra.load_data(f\"{data_folder}{filename_deconv_subset}\", key=key)\n",
    "                print(\"Neural data loaded\")\n",
    "\n",
    "            except:\n",
    "                print(f\"Could not neural data from file {data_folder}{filename_deconv_subset} under key {key}.\")\n",
    "                try:\n",
    "                    neural = cebra.load_data(filepath, key='rois/deconv')\n",
    "                    print(\"Full neural data loaded from HDD file.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Could not load data.\")\n",
    "                    print(e)\n",
    "            \n",
    "            # auxiliary variables\n",
    "            stim_on_frames = cebra.load_data(f'{data_folder}{filename_stim_pres_frames}', key=\"stim_on_frames\")\n",
    "            print(f\"Stimulus presentation frames loaded.\")\n",
    "            print(\"All data loaded.\")\n",
    "\n",
    "# end else\n",
    "\n",
    "# format the discrete variable\n",
    "# left spot == 1, right spot == 2, no stimulus == 0 \n",
    "left_spot, right_spot = stim_on_frames[:,0], stim_on_frames[:,1]\n",
    "right_spot = np.multiply(right_spot, 2)\n",
    "discrete = np.add(left_spot, right_spot)\n",
    "\n",
    "# separate data into training and test\n",
    "training_test_split = TRAINING_TEST_SPLIT\n",
    "split_idx = int(np.round(neural.shape[0] * training_test_split))\n",
    "neural_train, neural_test = neural[:split_idx, :], neural[split_idx:, :]\n",
    "discrete_train, discrete_test = discrete[:split_idx], discrete[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load, fit, and save model ###\n",
    "# if LOAD_MODEL == True, load the specified model. Otherwise, fit the model\n",
    "# and additionally save it if SAVE_MODEL == True\n",
    "\n",
    "modelpath_root = MODELPATH_ROOT\n",
    "modelpath_date = MODELPATH_DATE\n",
    "\n",
    "# create model name and full model path\n",
    "model_name = 'f1_221103_spot-label_10000rois_2000iters_128hidden_1e-4L2'\n",
    "model_name = f\"{filename}_{LABEL}{'-label' if parameters['conditional'] == 'time_delta' else 'time'}_\" + \\\n",
    "    f\"{EXTRA_LABEL+'_' if EXTRA_LABEL else ''}\" + \\\n",
    "    f\"{ROIS}rois_{ITERS}iters_{parameters['num_hidden_units']}hidden_{parameters['weight_decay']}L2_\" + \\\n",
    "    f\"{TRAINING_TEST_SPLIT}train\"\n",
    "full_model_path = f\"{modelpath_root}{modelpath_date}{model_name}/{model_name}_{MODEL_VER}.pt\"\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "\n",
    "    ## load ##\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    cebra_stim_on_model = cebra.CEBRA.load(full_model_path)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "\n",
    "        ## save ##\n",
    "        print(\"Saving model...\")\n",
    "        cebra_stim_on_model.save(full_model_path)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    ## fit ## \n",
    "\n",
    "    print(\"Fitting model...\")\n",
    "    print(cebra_stim_on_model)\n",
    "\n",
    "\n",
    "    cebra_stim_on_model.fit(neural_train, discrete_train)\n",
    "    print(\"Model fit.\")\n",
    "\n",
    "\n",
    "if SAVE_MODEL and not LOAD_MODEL:\n",
    "    \n",
    "    ## save ##\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    # create the directory if it does not already exist (and continue if it does)\n",
    "    Path(f\"{modelpath_root}{modelpath_date}{model_name}\").mkdir(parents=True, exist_ok=True)\n",
    "    cebra_stim_on_model.save(full_model_path)\n",
    "    print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate/plot embeddings ###\n",
    "\n",
    "# CEBRA's plot_embedding function is bugged\n",
    "# fig = plt.figure(figsize=(6,12)\n",
    "# ax1 = plt.subplot(211)\n",
    "# ax2 = plt.subplot(212)\n",
    "\n",
    "# trained embedding\n",
    "embedding_train = cebra_stim_on_model.transform(neural_train)\n",
    "cebra.plot_embedding(embedding=embedding_train, embedding_labels=discrete_train, cmap='cool', title=\"Training Embedding\")\n",
    "\n",
    "# test embedding\n",
    "embedding_test = cebra_stim_on_model.transform(neural_test)\n",
    "cebra.plot_embedding(embedding=embedding_test, embedding_labels=discrete_test, cmap='cool', title=\"Test embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot loss and temperature ###\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "cebra.plot_loss(cebra_stim_on_model, ax=ax1)\n",
    "cebra.plot_temperature(cebra_stim_on_model, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decode ###\n",
    "## rewritten to train on training set ##\n",
    "\n",
    "decoder = cebra.KNNDecoder()\n",
    "\n",
    "decoder.fit(embedding_train, discrete_train)\n",
    "\n",
    "predictions = decoder.predict(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualise ###\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.plot(discrete_test, label='ground truth')\n",
    "# ax1.plot(predictions, label='prediction')\n",
    "ax1.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax2.plot(predictions, label='prediction')\n",
    "ax2.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax3.plot(discrete_test, label='ground truth')\n",
    "ax3.plot(predictions, label='predictions', alpha=0.6)\n",
    "ax3.legend(loc='upper right', fontsize='small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate decoder performance - f1 score ###\n",
    "# Also provide a lenient f1-score (where any prediction of a stimulus within \n",
    "# the stimulus duration counts as a fully accurate predictions)\n",
    "\n",
    "## convert left and right spots to the same value (1) ## \n",
    "\n",
    "# find the test subset\n",
    "training_test_split = TRAINING_TEST_SPLIT   \n",
    "split_idx = int(np.round(neural.shape[0] * training_test_split))\n",
    "# index stim_on mask to restrict data to the test subset\n",
    "left_spot, right_spot = stim_on_frames[split_idx:,0], stim_on_frames[split_idx:,1]\n",
    "# combine left and right spot stim_on mask\n",
    "discrete_test_f1 = np.add(left_spot, right_spot)\n",
    "\n",
    "# convert predictions to also all be the same value\n",
    "idx_to_replace = np.where(np.isin(predictions, 2))[0]\n",
    "predictions_f1 = np.copy(predictions)\n",
    "predictions_f1[idx_to_replace] = 1\n",
    "\n",
    "## calculate f1-score ##\n",
    "\n",
    "f1_score = sklearn.metrics.f1_score(discrete_test_f1, predictions_f1)\n",
    "\n",
    "## calculate lenient f1-score ##\n",
    "\n",
    "stim_length = STIM_LENGTH_FRAMES\n",
    "leniency_range = stim_length*2\n",
    "predictions_f1_lenient = np.copy(predictions_f1)\n",
    "\n",
    "# find the indexes where a prediction was made\n",
    "prediction_made = np.where(np.isin(predictions_f1, 1))[0]\n",
    "\n",
    "# for each index\n",
    "for idx in prediction_made:\n",
    "\n",
    "    # check if this is a correct prediction (ground-truth also 1)\n",
    "    if discrete_test_f1[idx] == 1:\n",
    "        # for the leniency_range timepoints around the stimulus, copy any 1s from the ground truth to predictions\n",
    "        upper_bound, lower_bound = np.ceil(idx+leniency_range/2).astype(int), np.ceil(idx-leniency_range/2).astype(int)\n",
    "        predictions_f1_lenient[lower_bound:upper_bound] = discrete_test_f1[lower_bound:upper_bound]\n",
    "\n",
    "# find the lenient f1-score\n",
    "lenient_f1_score = sklearn.metrics.f1_score(discrete_test_f1, predictions_f1_lenient)\n",
    "\n",
    "\n",
    "## report f1-score ##\n",
    "\n",
    "print(f\"Strict f1-score:  {f1_score:.3f}\\nLenient f1-score: {lenient_f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_test_f1[2000] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate lenient f1-score ##\n",
    "\n",
    "stim_length = 22\n",
    "leniency_range = stim_length*2\n",
    "predictions_f1_lenient = predictions_f1\n",
    "\n",
    "# find the indexes where a prediction was made\n",
    "prediction_made = np.where(np.isin(predictions_f1, 1))[0]\n",
    "\n",
    "# for each index\n",
    "for idx in prediction_made:\n",
    "\n",
    "    # check if this is a correct prediction (ground-truth also 1)\n",
    "    if discrete_test_f1[idx] == 1:\n",
    "        # for the leniency_range timepoints around the stimulus, copy any 1s from the ground truth to predictions\n",
    "        upper_bound, lower_bound = np.ceil(idx+leniency_range/2).astype(int), np.ceil(idx-leniency_range/2).astype(int)\n",
    "        print(upper_bound, lower_bound)\n",
    "        predictions_f1_lenient[lower_bound:upper_bound] = discrete_test_f1[lower_bound:upper_bound]\n",
    "\n",
    "# find the lenient f1-score\n",
    "lenient_f1_score = sklearn.metrics.f1_score(discrete_test_f1, predictions_f1_lenient)\n",
    "print(lenient_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score, lenient_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_test_f1[920:950], predictions_f1[920:950]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise ### \n",
    "fig, ((ax1, ax2)) = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1.plot(discrete_test, label='ground truth')\n",
    "# ax1.plot(predictions, label='prediction')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(predictions, label='prediction')\n",
    "ax2.legend()\n",
    "\n",
    "fig2, ax3 = plt.subplots(1,1, figsize=(10,4))\n",
    "ax3.plot(discrete_test, label='ground truth')\n",
    "ax3.plot(predictions, label='predictions', alpha=0.6)\n",
    "ax3.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.plot(discrete_test, label='ground truth')\n",
    "# ax1.plot(predictions, label='prediction')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(predictions, label='prediction')\n",
    "ax2.legend()\n",
    "\n",
    "# fig2, ax3 = plt.subplots(1,1, figsize=(5,4))\n",
    "ax3.plot(discrete_test, label='ground truth')\n",
    "ax3.plot(predictions, label='predictions', alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.plot(discrete_test, label='ground truth')\n",
    "# ax1.plot(predictions, label='prediction')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(predictions, label='prediction')\n",
    "ax2.legend()\n",
    "\n",
    "ax3.plot(discrete_test, label='ground truth')\n",
    "ax3.plot(predictions, label='predictions', alpha=0.6)\n",
    "ax3.legend(loc='upper right', fontsize='small')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
