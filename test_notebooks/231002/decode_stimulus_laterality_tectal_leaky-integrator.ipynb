{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPIM data CEBRA model\n",
    "\n",
    "- Use CEBRA label contrastive learning on neural data from one fish\n",
    "    - design model\n",
    "    - load data\n",
    "    - SPIM data preprocessing\n",
    "    - fit with label of the stimulus\n",
    "    - plot embeddings\n",
    "    - decode stimulus presence and type (left/right spots)<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, running time-since-last, to analyse stimulus-triggered response\n",
    "\n",
    "### For the script, will be using cebra.CEBRA.partial_fit() and assessing prediction accuracy with each run (root MSE of continuous variable difference)\n",
    "\n",
    "### Implementing a 'leaky integrator' continuous representation to replace the linear time-since-last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import cebra\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set random seeds ###\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define globals ### \n",
    "\n",
    "# list of all data files\n",
    "dat_files = ['/media/storage/DATA/lfads_export/f1_221027.h5',\n",
    "             '/media/storage/DATA/lfads_export/f1_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f2_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f3_221103.h5']\n",
    "\n",
    "global SAMPLING_TYPE           # either 'time', 'time_delta', or 'delta' (delta minimally implemented)\n",
    "global LABEL                   # brief description of model training label\n",
    "global EXTRA_LABEL             # optional extra information to include in model name\n",
    "global FILEPATH                # path to data file\n",
    "global DATA_PATH               # path to experimental data\n",
    "global MODELPATH_ROOT          # folder path for models folder\n",
    "global MODELPATH_DATE          # specific path within models folder\n",
    "global MODEL_VER               # append version to model name for when saving multiple repeats\n",
    "global FRAMERATE               # framerate of neural recording, Hz\n",
    "global TIMESTEPS               # no. of neural timepoints to use\n",
    "global ROIS                    # no. of ROIs to use \n",
    "global ITERS                   # model training iterations\n",
    "global PARTIAL_FIT             # train the model in specified incremental step. As the model can be saved and loaded as\n",
    "                               # usual, can be combined with LOAD_MODEL to repeatedly train\n",
    "global LOAD_DATA               # attempt to load pre-saved .npz data files\n",
    "global SAVE_DATA               # save new .npz files when created\n",
    "global LOAD_MODEL              # attempt to load model\n",
    "global SAVE_MODEL              # save model to models folder\n",
    "global SAVE_FIGS               # automatically save figures generated\n",
    "global SAVE_FIGS_LABEL         # extra label to be added to figure names as they are saved\n",
    "global STIM_TYPES              # dictionary of stim types to stim numbers\n",
    "global STIMS                   # relevant stims for model\n",
    "global TRAINING_TEST_SPLIT     # split fraction for embedding model\n",
    "global STIM_LENGTH_FRAMES      # the length in frames of stimuli, assuming all stimuli have equal duration\n",
    "global HALF_STIM_MASK          # only use the second half of stimuli for training the model\n",
    "global DISCRETE_AND_CONT_STIM  # combine the discrete stimulus identifier variable with an linearly increasing \n",
    "                               # continuous variable.\n",
    "                               # continuous variable is currently independent of other flags. It will e.g. not \n",
    "                               # be influenced by HALF_STIM_MASK\n",
    "global PREDICT_ON_TRAIN        # for decoder, do both training and prediciton on the training set\n",
    "global RESTRICT_TO_TECTAL      # get random subset of neurons from the tectal mask only\n",
    "global TRIAL_TIME              # If True, use concatenated peri-trial timeseries instead of continuous timeseries\n",
    "global SIGNAL_TYPE             # whether to use deconvolved calcium or raw calcium\n",
    "global RANDOMISED_TEST         # when running on discontinuous data, randomise the order of the 'trials' for a \n",
    "                               # balanced test set\n",
    "global TIME_SINCE_LAST         # continuous variable recording the time since last stimulus (of that type).\n",
    "                               # not compatible with the discrete and continuous stim labelling\n",
    "global LEAKY_INTEGRATOR        # continuous variable convolving a decaying exponential with stimulus onset frames\n",
    "\n",
    "SAMPLING_TYPE = 'time_delta'\n",
    "LABEL = 'spot'                                                          # set as '' for time-contrastive\n",
    "EXTRA_LABEL = 'second-half-stims_leaky-integrator_tectal'\n",
    "FILEPATH = dat_files[1]\n",
    "DATA_PATH = '/home/tomh/Documents/projects/czebra/test_notebooks/data/'\n",
    "MODELPATH_ROOT = '/home/tomh/Documents/projects/czebra/archived_models/'\n",
    "MODELPATH_DATE = '231002/'\n",
    "MODEL_VER = 10\n",
    "FRAMERATE = 5\n",
    "TIMESTEPS = None                                                        # set as None to use all timepoints\n",
    "ROIS =  10000 #int(101435 * 0.50) \n",
    "ITERS = 500 # 2000\n",
    "PARTIAL_FIT = False                                                     # set False to ignore, else max_iters\n",
    "LOAD_DATA = True\n",
    "LOAD_MODEL = False\n",
    "SAVE_DATA = False\n",
    "SAVE_MODEL = True\n",
    "SAVE_FIGS = True\n",
    "STIM_TYPES = {'left_spot':0, 'right_spot':1,  \\\n",
    "              'open_loop_grating':2, 'closed_loop_grating':3}\n",
    "STIMS = ['left_spot', 'right_spot']\n",
    "TRAINING_TEST_SPLIT = 0.8\n",
    "STIM_LENGTH_FRAMES = 22\n",
    "HALF_STIM_MASK = True\n",
    "DISCRETE_AND_CONT_STIM = False       \n",
    "PREDICT_ON_TRAIN = False\n",
    "RESTRICT_TO_TECTAL = True\n",
    "SAVE_FIGS_LABEL = 'predict-on-train_' if SAVE_FIGS and PREDICT_ON_TRAIN else ''\n",
    "TRIAL_TIME = False\n",
    "SIGNAL_TYPE = 'deconv'        # dfof for raw, deconv for deconvolved\n",
    "RANDOMISED_TEST = False\n",
    "TIME_SINCE_LAST = False\n",
    "LEAKY_INTEGRATOR = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading data, the notebook will try to load the generic dataset with the name given by the chosen combination of flags. This is similar for loading the model, but it will also take into account the \"extra label\", to string match to the right model folder. Remember to set the extra label to match the chosen flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model ###\n",
    "\n",
    "parameters = {'conditional':SAMPLING_TYPE,\n",
    "              'max_iterations':ITERS,\n",
    "              'batch_size':1024,\n",
    "              'num_hidden_units':128,\n",
    "              'weight_decay':1e-03,\n",
    "              'rois':ROIS,\n",
    "             }\n",
    "\n",
    "cebra_stim_on_model = cebra.CEBRA(\n",
    "    model_architecture='offset10-model',\n",
    "    device='cuda_if_available',\n",
    "    conditional=parameters['conditional'],\n",
    "    temperature_mode='auto',\n",
    "    min_temperature=0.1,\n",
    "    time_offsets=10,\n",
    "    max_iterations= parameters['max_iterations'] if not PARTIAL_FIT else PARTIAL_FIT,\n",
    "    max_adapt_iterations=500,\n",
    "    batch_size=parameters['batch_size'],\n",
    "    learning_rate=1e-4,\n",
    "    output_dimension=3,\n",
    "    verbose=True,\n",
    "    num_hidden_units=parameters['num_hidden_units'],\n",
    "    hybrid=False,\n",
    "    optimizer_kwargs=(('betas', (0.9, 0.999)), ('eps', 1e-08), ('weight_decay', parameters['weight_decay']), ('amsgrad', False))\n",
    "    )\n",
    "print(cebra_stim_on_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for loading, saving and preprocessing model input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neural_dataset(f, neural, rois, timesteps=None, restrict_to_tectal=None):  \n",
    "    ''' Take a random sample of rois from either the tectal-restricted dataset or\n",
    "        the full dataset. \n",
    "        If a number of timesteps are given, only index the neural data up to this value\n",
    "        f                   - h5pyFile object for loading\n",
    "        neural              - neural dataset from above file\n",
    "        rois                - num rois to choose\n",
    "        timesteps           - maximum index of neural in time dimension\n",
    "        restrict_to_tectal  - flag for restricting dataset to tectum only'''\n",
    "    \n",
    "    # decide whether to sample from all neurons, or tectal neurons only\n",
    "    if restrict_to_tectal:\n",
    "        \n",
    "        print(\"Taking tectal neurons only.\")\n",
    "        tectal_mask = f['rois']['anat_label']['tectum_SPV_AZ'][:]\n",
    "        neural_indexes = np.sort(\n",
    "                            np.random.choice(\n",
    "                                            np.where(tectal_mask)[0], size=rois, replace=False\n",
    "                                            )\n",
    "                                )\n",
    "    else:\n",
    "        \n",
    "        # subset neural by selecting first TIMESTEPS timesteps and random ROIS rois\n",
    "        neural_indexes = np.sort(\n",
    "                            np.random.choice(\n",
    "                                        np.arange(neural.shape[1]), size=rois, replace=False\n",
    "                                        )\n",
    "                            )\n",
    "    \n",
    "    # if a subset of timesteps is chosen, restrict data to this subset\n",
    "    if timesteps:\n",
    "        neural = np.array(neural[:timesteps, neural_indexes])\n",
    "    else:\n",
    "        neural = np.array(neural[:, neural_indexes])\n",
    "        timesteps = neural.shape[0]\n",
    "\n",
    "    print(f\"Truncated dataset size: {neural.shape}\")\n",
    "    assert(neural.shape == (timesteps, rois))\n",
    "\n",
    "    return (neural, neural_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stimulus_presentation_masks(f, neural, stims, stim_types, stim_length_frames, timesteps=None):\n",
    "    ''' For every input stimulus type, find:\n",
    "    - index value of the stimulus presentations\n",
    "    - frame index of stimulus onset\n",
    "    - frame index of stimulus end\n",
    "    - the boolean mask for when this stimulus is being presented\n",
    "    - the above 3 for the second half of the stimulus duration\n",
    "    - the stimulus duration\n",
    "      \n",
    "    Second-half stimulus found by adding half of stimulus duration to onset frame numbers\n",
    "    f                   - h5pyFile object for loading\n",
    "    neural              - neural dataset from above file\n",
    "    stims               - chosen stim types for this extraction (ArrayLike of ints for indexing stim_types)\n",
    "    stim_types          - dict of stimulus types, indexed by stims\n",
    "    stim_length_frames  - duration of stimuli (currently fixed for all stimuli)\n",
    "    timesteps           - maximum index of neural in time dimension\n",
    "    '''\n",
    "\n",
    "\n",
    "     # get stimulus presentations\n",
    "    stimuli = f['visuomotor']['presentations']\n",
    "    stim_type = stimuli['stim_type'][:timesteps].astype(int)\n",
    "    stim_on_fr = stimuli['onset_frame'][:timesteps].astype(int)\n",
    "    stim_end_fr = stimuli['offset_frame'][:timesteps].astype(int)\n",
    "\n",
    "    # initialise lists for the chosen stimuli\n",
    "    (stim_pres_idx_list, stim_on_fr_list, \n",
    "    stim_on_fr_list_half,\n",
    "    stim_end_fr_list, stim_on_mask_list, \n",
    "    stim_on_mask_list_half,\n",
    "    stim_dur_list)                        = [],[],[],[],[],[],[]\n",
    "\n",
    "\n",
    "    # loop through chosen stimuli and find boolean masks for their 'on' frames\n",
    "    for stim in stims:\n",
    "\n",
    "        # convert stim name to stim number\n",
    "        stim_num = stim_types[stim] \n",
    "        print(f'Attempting to parse stim: {stim}') \n",
    "\n",
    "        # find the presentation indexes for the specified stim type\n",
    "        # must account for data index starting at 1\n",
    "        this_stim_pres_indexes = np.where(np.isin(stim_type, stim_num + 1))[0]\n",
    "        stim_pres_idx_list.append(this_stim_pres_indexes)\n",
    "\n",
    "        # index stim-specific onset frame numbers \n",
    "        this_stim_on_frames = stim_on_fr[this_stim_pres_indexes]\n",
    "        if timesteps:\n",
    "            this_stim_on_frames = this_stim_on_frames[this_stim_on_frames < timesteps]\n",
    "        stim_on_fr_list.append(this_stim_on_frames)\n",
    "\n",
    "        # Find also the second half of stimulus frames: increment the start frame\n",
    "        # indexes by half the stimulus duration, to simulate the stimulus starting halfway\n",
    "        # later than it actually does\n",
    "        this_stim_on_frames_half = (this_stim_on_frames + stim_length_frames/2).astype(int)\n",
    "        stim_on_fr_list_half.append(this_stim_on_frames_half)\n",
    "\n",
    "        # index stim-specific end frame numbers \n",
    "        this_stim_end_frames = stim_end_fr[this_stim_pres_indexes]\n",
    "        if timesteps:\n",
    "            this_stim_end_frames = this_stim_end_frames[this_stim_end_frames < timesteps]\n",
    "        stim_end_fr_list.append(this_stim_end_frames)\n",
    "            \n",
    "\n",
    "        # create a boolean mask of stimulus presentation frames (1 == stimulus on, 0 == stimulus off)\n",
    "        # do this for second-half-stim and full stim\n",
    "        this_stim_on_mask = np.zeros(neural.shape[0]).astype(int)\n",
    "        this_stim_on_mask[[this_stim_on_frames, this_stim_end_frames]] = 1\n",
    "        this_stim_on_mask_half = np.zeros(neural.shape[0]).astype(int)\n",
    "        this_stim_on_mask_half[[this_stim_on_frames_half, this_stim_end_frames]] = 1\n",
    "        \n",
    "        # perform bitwise XOR operation on consecutive elements of stim_on_mask. This will convert all \n",
    "        # but stim_off frame to 1s. Combining with \"OR stim_on_mask\" will also include the stim_off frame\n",
    "        this_stim_on_mask = np.bitwise_xor.accumulate(this_stim_on_mask) | this_stim_on_mask\n",
    "        stim_on_mask_list.append(this_stim_on_mask)\n",
    "        this_stim_on_mask_half = np.bitwise_xor.accumulate(this_stim_on_mask_half) | this_stim_on_mask_half\n",
    "        stim_on_mask_list_half.append(this_stim_on_mask_half)\n",
    "        \n",
    "        # find duration (in frames) of each presentation of the stimulus\n",
    "        # recording rate is 5 Hz\n",
    "        stim_dur_list.append(this_stim_end_frames - this_stim_on_frames)\n",
    "\n",
    "        # assert shapes\n",
    "        print(f\"Label shape: {stim_on_mask_list[0].shape}\")\n",
    "        print(f'Stim type {stim} parsed successfully.')\n",
    "\n",
    "    return (stim_pres_idx_list, stim_on_fr_list, stim_on_fr_list_half,\n",
    "            stim_end_fr_list, stim_on_mask_list, stim_on_mask_list_half,\n",
    "                stim_dur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_file(stim_on_mask_dataset, neural, neural_indexes, \n",
    "              data_folder, filename_stim_pres_frames, filename_neural_subset,\n",
    "              filename_neural_indexes):   \n",
    "    ''' Save data to the specified folder. This includes neural data, indexes of \n",
    "        neural ROIs, and a binary mask of stimulus activity\n",
    "        stim_on_mask_dataset    - binary mask of stimulus activity (all stims)\n",
    "        neural                  - neural dataset from above file\n",
    "        neural_indexes          - index values of rois used in the dataset (w.r.t raw data)\n",
    "        data folder              - maximum index of neural in time dimension\n",
    "        filename_stim_pres_frames - flag for restricting dataset to tectum only\n",
    "        data folder              - maximum index of neural in time dimension\n",
    "        filename_stim_pres_frames - flag for restricting dataset to tectum only\n",
    "    '''\n",
    "\n",
    "    np.savez(f'{data_folder}{filename_stim_pres_frames}', stim_on_frames=stim_on_mask_dataset)\n",
    "    print(f\"Stim presentation dataset saved.\")\n",
    "    np.savez(f\"{data_folder}{filename_neural_subset}\", neural=neural)\n",
    "    print(f\"Neural dataset saved.\")\n",
    "    np.savez(f\"{data_folder}{filename_neural_indexes}\", neural_indexes=neural_indexes)\n",
    "    print(f\"All datasets saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(filepath, data_folder, filename_neural_subset,\n",
    "                        filename_stim_pres_frames):\n",
    "    ''' Attempt to load .npz files for neural data and auxiliary variables'''\n",
    "    \n",
    "    key = \"neural\"\n",
    "    try:\n",
    "        # loading dataset from SSD (not HDD) here\n",
    "        neural = cebra.load_data(f\"{data_folder}{filename_neural_subset}\", key=key)\n",
    "        print(\"Neural data loaded\")\n",
    "\n",
    "    except:\n",
    "        print(f\"Could not neural data from file {data_folder}{filename_neural_subset} under key {key}.\")\n",
    "        try:\n",
    "            # try loading directly from the .h5 file\n",
    "            neural = cebra.load_data(filepath, key='rois/deconv')\n",
    "            print(\"Full neural data loaded from HDD file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # give up\n",
    "            print(\"Could not load data.\")\n",
    "            print(e)\n",
    "    \n",
    "    # auxiliary variables\n",
    "    stim_on_frames = cebra.load_data(f'{data_folder}{filename_stim_pres_frames}', key=\"stim_on_frames\")\n",
    "    print(f\"Stimulus presentation frames loaded.\")\n",
    "    print(\"All data loaded.\")\n",
    "\n",
    "    return (neural, stim_on_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load and preprocess data for a single fish ###\n",
    "# if LOAD == True, load pre-saved .npz file data. Otherwise,\n",
    "# create this data as specified below and save it to .npz\n",
    "\n",
    "##  params ##\n",
    "\n",
    "# variables\n",
    "stim_types = STIM_TYPES     # dict of all possible stims\n",
    "stims = STIMS               # stim types chosen for analysis\n",
    "timesteps = TIMESTEPS\n",
    "rois = ROIS\n",
    "stim_length_frames = STIM_LENGTH_FRAMES # used for selecting the second half of stimuli\n",
    "\n",
    "load_data = LOAD_DATA\n",
    "save_data = SAVE_DATA\n",
    "\n",
    "# paths\n",
    "filepath = FILEPATH\n",
    "filename = filepath.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = DATA_PATH\n",
    "data_folder_HDD = '/media/storage/DATA/tom/'\n",
    "filename_stim_pres_frames = f'{filename[-9:]}_stim_pres_frames.npz'\n",
    "filename_neural_subset = f'{filename[-9:]}_{SIGNAL_TYPE}_subset.npz'\n",
    "filename_neural_indexes = f'{filename[-9:]}_neural_indexes_all.npz'\n",
    "\n",
    "# specify loading anatomically unrestricted data or tectal-restricted data\n",
    "if RESTRICT_TO_TECTAL:\n",
    "    filename_neural_subset = f'{filename[-9:]}_{SIGNAL_TYPE}_subset_tectal.npz'\n",
    "    filename_neural_indexes = f'{filename[-9:]}_neural_indexes_tectal.npz'\n",
    "\n",
    "# if not loading data, but not wanting to overwrite saved data, save as a temp file\n",
    "if not save_data and not load_data: \n",
    "    print(f\"Producing temp files...\")\n",
    "    filename_neural = f'{filename[-9:]}_{SIGNAL_TYPE}_TEMPORARY_DELETE.npz'\n",
    "    filename_neural_subset = f'{filename[-9:]}_{SIGNAL_TYPE}_subset_TEMPORARY_DELETE.npz'\n",
    "    filename_stim_pres_frames = f'{filename[-9:]}_stim_pres_frames_TEMPORARY_DELETE.npz'\n",
    "\n",
    "\n",
    "print(\"Accessing data...\")\n",
    "\n",
    "## load data ##\n",
    "if load_data:\n",
    "        \n",
    "    # Attempt to load neural data from .npz, otherwise load from HDD .h5\n",
    "    # Load small datasets from .npz files\n",
    "    print(\"Loading data...\")\n",
    "    (neural, stim_on_frames) =  load_data_from_file(filepath, data_folder, filename_neural_subset,\n",
    "                                                    filename_stim_pres_frames)\n",
    "\n",
    "\n",
    "## else generate data ##\n",
    "else:\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "            ## neural ##\n",
    "\n",
    "            neural_dataset = f['rois'][f'{SIGNAL_TYPE}']\n",
    "            print(f\"Full neural dataset shape is: {neural_dataset.shape}\")\n",
    "\n",
    "            neural, neural_indexes = generate_neural_dataset(f, neural_dataset, rois, timesteps=timesteps,\n",
    "                                                             restrict_to_tectal=RESTRICT_TO_TECTAL)\n",
    "\n",
    "            ## stimuli ##\n",
    "\n",
    "            (stim_pres_idx_list, \n",
    "             stim_on_fr_list,\n",
    "             stim_on_fr_list_half,\n",
    "             stim_end_fr_list, \n",
    "             stim_on_mask_list, \n",
    "             stim_on_mask_list_half,\n",
    "             stim_dur_list) = create_stimulus_presentation_masks(f, neural, stims, stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "\n",
    "            ## save data ##\n",
    "            print(\"Saving data...\")\n",
    "\n",
    "            # choose which stim_on_mask to use (half or full)\n",
    "            # stim_on_mask half will ignore the first half of stimuli for the rest\n",
    "            # of the session\n",
    "            if HALF_STIM_MASK:\n",
    "                stim_on_mask_dataset = np.column_stack(stim_on_mask_list_half[:])\n",
    "            else: \n",
    "                 stim_on_mask_dataset = np.column_stack(stim_on_mask_list[:])\n",
    "\n",
    "            assert(stim_on_mask_dataset.shape == (neural.shape[0], len(stims)))\n",
    "            if timesteps:\n",
    "                assert(neural.shape == (timesteps, rois))\n",
    "\n",
    "            save_data_to_file(stim_on_mask_dataset, neural, neural_indexes, \n",
    "              data_folder, filename_stim_pres_frames, filename_neural_subset,\n",
    "              filename_neural_indexes)\n",
    "\n",
    "            ## load data ##\n",
    "            # Attempt to load neural data from .npz, otherwise load from HDD .h5\n",
    "            # Load small datasets from .npz files\n",
    "            print(\"Loading data...\")\n",
    "            \n",
    "            (neural, stim_on_frames) = load_data_from_file(filepath, data_folder, filename_neural_subset,\n",
    "                                                           filename_stim_pres_frames)\n",
    "# end else   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data processing ###\n",
    "\n",
    "# format the discrete variable\n",
    "# left spot == 1, right spot == 2, no stimulus == 0 \n",
    "left_spot, right_spot = stim_on_frames[:,0], stim_on_frames[:,1]\n",
    "right_spot = np.multiply(right_spot, 2)\n",
    "discrete = np.add(left_spot, right_spot)\n",
    "\n",
    "# separate data into training and test\n",
    "training_test_split = TRAINING_TEST_SPLIT\n",
    "split_idx = int(np.round(neural.shape[0] * training_test_split))\n",
    "neural_train, neural_test = neural[:split_idx, :], neural[split_idx:, :]\n",
    "discrete_train, discrete_test = discrete[:split_idx], discrete[split_idx:]\n",
    "\n",
    "# if set, create a separate continuous \"contrast\" variable for each stimulus, to train\n",
    "# the CEBRA model on alongside the discrete variable\n",
    "# This is INDEPENDENT of other flags. It will always give the true linear ramp from \n",
    "# stimulus onset to end\n",
    "if DISCRETE_AND_CONT_STIM:\n",
    "    \n",
    "    print(f\"Creating continuous 'contrast' variable for stimuli...\")\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        (_,stim_on_fr_list,_,_,_,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                        stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "        \n",
    "    left_spot_cont = np.zeros(neural.shape[0])\n",
    "    right_spot_cont = np.zeros(neural.shape[0])\n",
    "    cont_stimuli = [left_spot_cont, right_spot_cont]\n",
    "    for i in range(len(cont_stimuli)):\n",
    "        this_stim_on_fr = stim_on_fr_list[i]\n",
    "        for pres in this_stim_on_fr:\n",
    "            cont_stimuli[i][pres:pres+STIM_LENGTH_FRAMES] = np.arange(STIM_LENGTH_FRAMES)\n",
    "\n",
    "    # separate data into training and test\n",
    "    left_spot_cont_train, left_spot_cont_test =  left_spot_cont[:split_idx], left_spot_cont[split_idx:]\n",
    "    right_spot_cont_train, right_spot_cont_test = right_spot_cont[:split_idx], right_spot_cont[split_idx:]\n",
    "    \n",
    "    # arange as expected input to CEBRA\n",
    "    cont_test = np.vstack((left_spot_cont_test, right_spot_cont_test)).T\n",
    "    cont_train = np.vstack((left_spot_cont_train, right_spot_cont_train)).T\n",
    "\n",
    "    print(f\"Created.\")\n",
    "\n",
    "# if set, create a new array for each stimulus which counts up the indices since the last presentation\n",
    "# of this stimulus\n",
    "if TIME_SINCE_LAST:\n",
    "\n",
    "    print(f\"Creating continuous 'time-since-last' variable for stimuli...\")\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        (_,stim_on_fr_list,_,_,_,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                        stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "    \n",
    "    # initialise counting arrays\n",
    "    time_since_last_spot_l = np.zeros(discrete.size)\n",
    "    time_since_last_spot_r = np.zeros(discrete.size)\n",
    "    time_since_last_stim_list = [time_since_last_spot_l, time_since_last_spot_r]\n",
    "\n",
    "    # for each stimulus type\n",
    "    for stim in range(len(stim_on_fr_list)):\n",
    "        this_stim_on_fr = stim_on_fr_list[stim]\n",
    "        time_since_last_stim = time_since_last_stim_list[stim]\n",
    "        # loop through each presentation of stim in order\n",
    "        for pres in this_stim_on_fr:\n",
    "            # create counter\n",
    "            counter = np.arange(pres, discrete.size)\n",
    "            # overwrite the counting array with the counter\n",
    "            # this will start a new count at the index of each stimulus presentation,\n",
    "            # and continue counting until the end of the session, unless overwritten later\n",
    "            # in the loop by another stimulus presentation\n",
    "            time_since_last_stim[pres:] = counter - pres\n",
    "\n",
    "    # separate data into training and test\n",
    "    left_spot_cont_train, left_spot_cont_test = time_since_last_spot_l[:split_idx], time_since_last_spot_l[split_idx:]\n",
    "    right_spot_cont_train, right_spot_cont_test = time_since_last_spot_r[:split_idx], time_since_last_spot_r[split_idx:]\n",
    "\n",
    "    ## remove time before the first round of stimulus presentations (as 0 time-since-last \n",
    "    ## doesn't make logical sense\n",
    "    # identify the end of 0 frames\n",
    "    first_stim_pres = [stim_on_fr_list[i][0] for i in range(len(stim_on_fr_list))]\n",
    "    latest_of_first_stim_pres = max(first_stim_pres)\n",
    "\n",
    "    # truncate start of relevant data\n",
    "    # specifically, neural data and stimulus count data (and discrete just in-case)\n",
    "    # time_since_last_stim_list = [time_since[latest_of_first_stim_pres:] for time_since in time_since_last_stim_list]\n",
    "    left_spot_cont_train = left_spot_cont_train[latest_of_first_stim_pres:]\n",
    "    right_spot_cont_train = right_spot_cont_train[latest_of_first_stim_pres:]\n",
    "    neural_train = neural_train[latest_of_first_stim_pres:]\n",
    "    discrete_train = discrete_train[latest_of_first_stim_pres:]\n",
    "\n",
    "\n",
    "    # arange as expected input to CEBRA\n",
    "    cont_test = np.vstack((left_spot_cont_test, right_spot_cont_test)).T\n",
    "    cont_train = np.vstack((left_spot_cont_train, right_spot_cont_train)).T\n",
    "\n",
    "# end if\n",
    "\n",
    "# If set, create a continuous variable that convolves a decaying exponential with stimulus\n",
    "# onset times\n",
    "if LEAKY_INTEGRATOR:\n",
    "\n",
    "    ## define discrete \"stim_onset_mask\" variable to be convolved ##\n",
    "\n",
    "    print(f\"Creating continuous 'leaky integrator' variable for stimuli...\")\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        (_,_,stim_on_fr_list_half,_,_,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                        stim_types,\n",
    "                                                                stim_length_frames, timesteps)\n",
    "        \n",
    "    # use the half-stimulus-delayed stim onset frames in place of the true stim onset frames\n",
    "    stim_on_fr_list = stim_on_fr_list_half\n",
    "        \n",
    "    ## REMOVED because I already implemented this in the create_stimulus_presentation_masks func\n",
    "    # # adjust stim_on_fr values so that the SECOND HALF of stims acts as the trigger\n",
    "    # half_stim_fr = FRAMERATE*2\n",
    "    # for i in range(len(stim_on_fr_list)):\n",
    "    #     stim_on_fr_list[i] = stim_on_fr_list[i] + half_stim_fr\n",
    "    \n",
    "    # use stimulus frame numbers to create a boolean array\n",
    "    stim_onset_mask = []\n",
    "    # loop through stim types\n",
    "    for i in range(len(stim_on_fr_list)):\n",
    "        this_stim_fr = stim_on_fr_list[i]\n",
    "        this_stim_onset_mask = np.zeros(neural.shape[0])\n",
    "        this_stim_onset_mask[this_stim_fr] = 1\n",
    "        stim_onset_mask.append(this_stim_onset_mask.astype(int))\n",
    "\n",
    "        \n",
    "    ## create decaying exponential kernel to convolve with ##\n",
    "    def exp_decay(x, param=0.1, n_zero=100):\n",
    "        \n",
    "        return n_zero*np.exp(-x*param)\n",
    "\n",
    "    # if kernel size is smaller than the largest gap between stimulus presentations,\n",
    "    # there will be a sudden discontinuity\n",
    "    # if kernel size is larger than the frame of the first presentation, the first\n",
    "    # presentation will be included in the non-valid data. \n",
    "    # Maybe I should set kernel size to be at least the largest gap between presentations\n",
    "    # and truncate the data to remove invalid values\n",
    "    # ALSO: the larger the kernel, the more data is considered invalid, and the smoother\n",
    "    # the convolution is\n",
    "    # UPDATE: For mode='valid', any points where the data and kernel do completely overlap are\n",
    "    # truncated. This means the first kernel_size datapoints, but then up to the max size of the\n",
    "    # dataset. For this case I should take 'full', remove kernel_size points from the end, and then\n",
    "    # remove points up until the first stimulus presentation\n",
    "    kernel_size = 20000\n",
    "    x_vals = np.arange(kernel_size)\n",
    "    decay_constant = (1e-2)/2\n",
    "    kernel = exp_decay(x_vals, param=decay_constant, n_zero=100)\n",
    "\n",
    "    ## do convolution ##\n",
    "\n",
    "    convolved_stim_onset_fr = []\n",
    "    # loop over stimuli\n",
    "    for i in range(len(stim_onset_mask)):\n",
    "        this_convolved_stim = np.convolve(stim_onset_mask[i], kernel, mode='full')\n",
    "        convolved_stim_onset_fr.append(this_convolved_stim)\n",
    "    print(\"Convolution complete.\")\n",
    "\n",
    "    ## process convolved and neural data ##\n",
    "    \n",
    "    # CONVOLVED\n",
    "    # loop over stimuli\n",
    "    for i in range(len(stim_onset_mask)):\n",
    "        # truncate any points added on past the end of the original signal\n",
    "        convolved_stim_onset_fr[i] = convolved_stim_onset_fr[i][:neural.shape[0]]\n",
    "        \n",
    "        # CURRENTLY REMOVED SO THAT ALL STIM CAN BE USED TO TRAIN A SINGLE MODEL SIMULTANEOUSLY\n",
    "        # # truncate points up until the first stimulus presentation\n",
    "        # first_stim_fr = stim_on_fr_list[i][0]\n",
    "        # convolved_stim_onset_fr[i] = convolved_stim_onset_fr[i][first_stim_fr:]\n",
    "\n",
    "        ## CURRENTLY REMOVED to reduce overcomplicating, and could be added in again later\n",
    "        # # remove time before the first round of stimulus presentations \n",
    "        # # as an alternative to tailoring the left-truncation to each stimulus\n",
    "        # first_stim_pres = [stim_on_fr_list[i][0] for i in range(len(stim_on_fr_list))]\n",
    "        # latest_of_first_stim_pres = max(first_stim_pres)\n",
    "        # convolved_stim_onset_fr[i] = convolved_stim_onset_fr[i][latest_of_first_stim_pres:]\n",
    "        \n",
    "        # # now readjust stim_on_fr_list so that it's indices are in the same representation as \n",
    "        # # the indices of the convolved data\n",
    "        # stim_on_fr_list[i] = stim_on_fr_list[i] - latest_of_first_stim_pres\n",
    "        # stim_on_fr_list[i] = stim_on_fr_list[i][stim_on_fr_list[i] > 0 ]\n",
    "\n",
    "    # arrange continuous variable as expected input to CEBRA\n",
    "    cont_var = np.vstack(convolved_stim_onset_fr).T\n",
    "\n",
    "    # separate into test and train\n",
    "    split_idx = int(TRAINING_TEST_SPLIT*cont_var.shape[0])\n",
    "    cont_train = cont_var[:split_idx, :]\n",
    "    cont_test = cont_var[split_idx:, :]\n",
    "\n",
    "    # NEURAL\n",
    "    ## CURRENTLY REMOVED to reduce overcomplicating, and could be added in again later\n",
    "    # remove time before the first round of stimulus presentations \n",
    "    # as an alternative to tailoring the left-truncation to each stimulus\n",
    "    # neural_leaky_integrator = neural[latest_of_first_stim_pres:]\n",
    "    neural_leaky_integrator = neural\n",
    "\n",
    "    # separate into test and train\n",
    "    neural_test = neural_leaky_integrator[split_idx:]\n",
    "    neural_train = neural_leaky_integrator[:split_idx]\n",
    "\n",
    "    # # assert dataset sizes\n",
    "    # # neural_test should have the correct training set size removed and the time until\n",
    "    # # the end of the first round of stimulus presentations removed.\n",
    "    assert neural_test.shape[0] == neural.shape[0] - split_idx\n",
    "\n",
    "    print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_test.shape, cont_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_var.shape, cont_train.shape, split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zero = 100\n",
    "param = 0.25\n",
    "x = np.arange(25)\n",
    "\n",
    "a = n_zero*np.exp(-x*param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create peri-stimulus \"trials\" ### \n",
    "\n",
    "if TRIAL_TIME:\n",
    "\n",
    "    ## params ##\n",
    "    pad = 4                      # time in s to pad either side of the stimulus in trial time\n",
    "    dur = STIM_LENGTH_FRAMES     # length of stimulus in frames\n",
    "    fr = FRAMERATE               # global recording framerate \n",
    "\n",
    "    # paths\n",
    "    # separate stim on mask and neural data for this trial structure\n",
    "    filename_neural_subset_trial_time = f'{filename[-9:]}_{SIGNAL_TYPE}_subset_trial_time.npz'\n",
    "    filename_stim_on_mask_trial_time = f'{filename[-9:]}_stim_on_mask_trial_time.npz'\n",
    "\n",
    "\n",
    "    ## access data ## \n",
    "\n",
    "    # create stimulus presentation masks, and find stimulus onset and end times\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "\n",
    "        (stim_pres_idx_list, \n",
    "            stim_on_fr_list,\n",
    "            stim_on_fr_list_half,\n",
    "            stim_end_fr_list, \n",
    "            stim_on_mask_list, \n",
    "            stim_on_mask_list_half,\n",
    "            stim_dur_list)          = create_stimulus_presentation_masks(f, neural, stims, stim_types,\n",
    "                                                                         stim_length_frames, timesteps=None)\n",
    "\n",
    "\n",
    "    # for each stimulus, combine stim on and off times into the same array\n",
    "    # (to get a general array of stimulus timings)\n",
    "    spot_stim_on_frames = np.sort(np.concatenate([stim_on_fr_list[0], stim_on_fr_list[1]]))\n",
    "    spot_stim_end_frames = np.sort(np.concatenate([stim_end_fr_list[0], stim_end_fr_list[1]]))\n",
    "\n",
    "    # separate out stimulus ON masks into left spot and right \n",
    "    left_spot, right_spot = np.copy(stim_on_mask_list[0]), np.copy(stim_on_mask_list[1])\n",
    "    left_spot_half, right_spot_half = np.copy(stim_on_mask_list_half[0]), np.copy(stim_on_mask_list_half[1])\n",
    "\n",
    "    ## crop data into peri-trial time only ##\n",
    "\n",
    "    # initialise numpy arrays (broadcast over rows)\n",
    "    neural_list, discrete_list_left, discrete_list_right = np.zeros((3, spot_stim_on_frames.size), dtype='object')\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        cont_list_left, cont_list_right = np.zeros((2, spot_stim_on_frames.size), dtype='object')\n",
    "\n",
    "    # for every stimulus presentation, crop neural and stimulus data to a window around the presentation\n",
    "    # defined by stimulation duration, data framerate, and amount of padding (pad)\n",
    "    for i in range(len(spot_stim_on_frames)):\n",
    "        pres = spot_stim_on_frames[i]\n",
    "\n",
    "        # neural\n",
    "        neural_list[i] = neural[pres - pad*fr: pres + dur + pad*fr, :]\n",
    "        \n",
    "        # left spot\n",
    "        # if taking second half of stims, crop the relevant array\n",
    "        if HALF_STIM_MASK:\n",
    "            discrete_list_left[i] = left_spot_half[pres - pad*fr: pres + dur + pad*fr]\n",
    "        else:\n",
    "            discrete_list_left[i] = left_spot[pres - pad*fr: pres + dur + pad*fr]\n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            cont_list_left[i] = left_spot_cont[pres -pad*fr: pres + dur + pad*fr]\n",
    "        \n",
    "        # right spot\n",
    "        # if taking second half of stims, crop the relevant array\n",
    "        if HALF_STIM_MASK:\n",
    "            discrete_list_right[i] = right_spot_half[pres - pad*fr: pres + dur + pad*fr]\n",
    "        else:   \n",
    "            discrete_list_right[i] = right_spot[pres - pad*fr: pres + dur + pad*fr]\n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            cont_list_right[i] = right_spot_cont[pres -pad*fr: pres + dur + pad*fr]\n",
    "\n",
    "    ## format data ##\n",
    "\n",
    "    # format the discrete variable by combining stimuli\n",
    "    # and set right spot value as 2 to differentiate from left spot\n",
    "    discrete_list = np.zeros(discrete_list_left.size, dtype='object')\n",
    "    for trial in range(discrete_list_left.size):\n",
    "        right_spot_trial = np.multiply(discrete_list_right[trial], 2)\n",
    "        discrete_list[trial] = np.add(discrete_list_left[trial], right_spot_trial)\n",
    "\n",
    "    ## save data ##\n",
    "\n",
    "    print(\"Saving trial-time data...\")\n",
    "    np.savez(f\"{data_folder}{filename_neural_subset_trial_time}\", neural_trial=neural_list)\n",
    "    np.savez(f\"{data_folder}{filename_stim_on_mask_trial_time}\", stim_on_mask=discrete_list)\n",
    "    print(f\"All datasets saved.\")\n",
    "\n",
    "    ## training-test split ##\n",
    "\n",
    "    # find the number of trials belonging to test and train sets\n",
    "    training_test_split = TRAINING_TEST_SPLIT\n",
    "    num_trials = int(len(neural_list))\n",
    "    num_train_trials = int(np.floor(training_test_split*num_trials))\n",
    "\n",
    "    # if only taking the last trials as test data\n",
    "    if not RANDOMISED_TEST:\n",
    "\n",
    "        # index lists with training and test indices\n",
    "        train_trials_idx = np.arange(num_train_trials)\n",
    "        test_trials_idx = np.arange(num_train_trials+1, num_trials)\n",
    "        neural_train, neural_test = np.concatenate(neural_list[train_trials_idx]), np.concatenate(neural_list[test_trials_idx])\n",
    "        discrete_train, discrete_test = np.concatenate(discrete_list[train_trials_idx]), np.concatenate(discrete_list[test_trials_idx])\n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            cont_left_test, cont_right_test = np.concatenate(cont_list_left[test_trials_idx]), \\\n",
    "                                                np.concatenate(cont_list_right[test_trials_idx])\n",
    "            cont_left_train, cont_right_train = np.concatenate(cont_list_left[train_trials_idx]), \\\n",
    "                                                np.concatenate(cont_list_right[train_trials_idx])\n",
    "\n",
    "    # if taking a random subset of trials as test data\n",
    "    elif RANDOMISED_TEST:\n",
    "\n",
    "        # select random trials equal to the test set size\n",
    "        num_test_trials = num_trials - num_train_trials\n",
    "        test_trials_idx = np.random.choice(np.arange(num_trials), size=num_test_trials)\n",
    "        # subtract the set of test trials from all trials to find the set of train trials\n",
    "        train_trials_idx =  list(set(np.arange(num_trials)) - set(test_trials_idx))\n",
    "\n",
    "        # index the lists of trials to create test and train data\n",
    "        (neural_test, \n",
    "            discrete_test) = (np.concatenate(neural_list[test_trials_idx]),\n",
    "                            np.concatenate(discrete_list[test_trials_idx]))\n",
    "        (neural_train, \n",
    "            discrete_train) = (np.concatenate(neural_list[train_trials_idx]),\n",
    "                            np.concatenate(discrete_list[train_trials_idx]))            \n",
    "        if DISCRETE_AND_CONT_STIM:\n",
    "            (cont_left_test,\n",
    "                cont_right_test) = (np.concatenate(cont_list_left[test_trials_idx]),\n",
    "                                    np.concatenate(cont_list_right[test_trials_idx]))\n",
    "            (cont_left_train,\n",
    "                cont_right_train) = (np.concatenate(cont_list_left[train_trials_idx]),\n",
    "                                    np.concatenate(cont_list_right[train_trials_idx]))\n",
    "    # end if/elif \n",
    "\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        # if using continuous variable, format it as expected by CEBRA\n",
    "        cont_test = np.vstack((cont_left_test, cont_right_test)).T\n",
    "        cont_train = np.vstack((cont_left_train, cont_right_train)).T\n",
    "\n",
    "else:\n",
    "    print(\"Continuous data only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRIAL_TIME:\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    # create a concatenated version for plotting\n",
    "    neural_trial = np.concatenate(neural_list)\n",
    "    discrete_trial = np.concatenate(discrete_list)\n",
    "    plt.plot(np.mean(neural_trial[:], axis=1), label='mean neural activity')\n",
    "    plt.plot(discrete_trial[:]/50, alpha=0.5, label='spot stimulus')\n",
    "    plt.legend(loc='upper right', fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRIAL_TIME:\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "    ax1.plot(np.mean(neural_list[45], axis=1))\n",
    "    ax2.plot(neural_list[45])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load, fit, and save model ###\n",
    "# if LOAD_MODEL == True, load the specified model. Otherwise, fit the model\n",
    "# and additionally save it if SAVE_MODEL == True\n",
    "\n",
    "modelpath_root = MODELPATH_ROOT\n",
    "modelpath_date = MODELPATH_DATE\n",
    "\n",
    "# create model name and full model path\n",
    "# model_name = 'f1_221103_spot-label_10000rois_2000iters_128hidden_1e-4L2'\n",
    "model_name = f\"{filename}_{LABEL}{'-label' if parameters['conditional'] == 'time_delta' else 'time'}_\" + \\\n",
    "    f\"{EXTRA_LABEL+'_' if EXTRA_LABEL else ''}\" + \\\n",
    "    f\"{ROIS}rois_{ITERS}iters_{parameters['num_hidden_units']}hidden_{parameters['weight_decay']}L2_\" + \\\n",
    "    f\"{TRAINING_TEST_SPLIT}train\"\n",
    "full_model_path = f\"{modelpath_root}{modelpath_date}{model_name}/{model_name}_{MODEL_VER}.pt\"\n",
    "model_dir = f\"{modelpath_root}{modelpath_date}{model_name}\"\n",
    "\n",
    "\n",
    "# if partial fitting the model and saving it, first load the model and continue with fitting, then save\n",
    "if SAVE_MODEL and PARTIAL_FIT:\n",
    "    \n",
    "    ## load ##\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    # Try to load the partial fit model. If it doesn't exist yet, skip this\n",
    "    # step and go straight to training\n",
    "    try:\n",
    "        cebra_stim_on_model = cebra.CEBRA.load(full_model_path)\n",
    "        print(\"Model loaded.\")\n",
    "    except:\n",
    "        print(\"Model not loaded. No model found; starting fresh training.\")\n",
    "\n",
    "\n",
    "    ## fit ## \n",
    "\n",
    "    print(\"Fitting model...\")\n",
    "    print(cebra_stim_on_model)\n",
    "\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        cebra_stim_on_model.partial_fit(neural_train, cont_train, discrete_train)\n",
    "        print(\"discrete and continuous stimulus model fit.\")\n",
    "\n",
    "    elif TIME_SINCE_LAST:\n",
    "        cebra_stim_on_model.partial_fit(neural_train, cont_train)\n",
    "        print(\"Time-since-last model fit.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Default training\")\n",
    "        cebra_stim_on_model.partial_fit(neural_train, discrete_train)\n",
    "        print(\"Discrete stimulus model fit.\")\n",
    "    \n",
    "elif LOAD_MODEL:  # do no fitting on a loaded model\n",
    "\n",
    "    ## load ##\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    cebra_stim_on_model = cebra.CEBRA.load(full_model_path)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "\n",
    "        ## save ##\n",
    "        print(\"Saving model...\")\n",
    "        cebra_stim_on_model.save(full_model_path)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "\n",
    "else:   # fit if not loading, or partial fitting\n",
    "\n",
    "    ## fit ## \n",
    "\n",
    "    print(\"Fitting model...\")\n",
    "    print(cebra_stim_on_model)\n",
    "\n",
    "    if DISCRETE_AND_CONT_STIM:\n",
    "        cebra_stim_on_model.fit(neural_train, cont_train, discrete_train)\n",
    "        print(\"discrete and continuous stimulus model fit.\")\n",
    "\n",
    "    elif TIME_SINCE_LAST:\n",
    "        cebra_stim_on_model.fit(neural_train, cont_train)\n",
    "        print(\"Time-since-last model fit.\")\n",
    "    \n",
    "    elif LEAKY_INTEGRATOR:\n",
    "        cebra_stim_on_model.fit(neural_train, cont_train)\n",
    "        print(\"Leaky integrator model fit.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Default training\")\n",
    "        cebra_stim_on_model.fit(neural_train, discrete_train)\n",
    "        print(\"Discrete stimulus model fit.\")\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "        \n",
    "        ## save ## \n",
    "\n",
    "        print(\"Saving model...\")\n",
    "        # create the directory if it does not already exist (and continue if it does)\n",
    "        Path(f\"{model_dir}\").mkdir(parents=True, exist_ok=True)\n",
    "        cebra_stim_on_model.save(full_model_path)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure to show the kernel used ###\n",
    "x_test = np.arange(1000)\n",
    "y_test = exp_decay(x_test, param=decay_constant, n_zero=1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test)\n",
    "\n",
    "os.mkdir(f\"{model_dir}/v{MODEL_VER}/\")\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}convolution-kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure to show mean neural activity against both the convolved\n",
    "  # continuous signal and the actual stimulus onset frame ###\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Creating continuous 'leaky integrator' variable for stimuli...\")\n",
    "with h5py.File(filepath, 'r') as f:\n",
    "    (_,temp_on_fr_list,temp_on_fr_list_half,_,_,_,_) = create_stimulus_presentation_masks(f, neural, stims, \n",
    "                                                                    stim_types,\n",
    "                                                            STIM_LENGTH_FRAMES, timesteps)\n",
    "\n",
    "min_frame = 3000\n",
    "max_frame = 6000\n",
    "left_stim_onsets = temp_on_fr_list[0][temp_on_fr_list[0] < max_frame]\n",
    "left_stim_onsets = left_stim_onsets[left_stim_onsets > min_frame]\n",
    "right_stim_onsets = temp_on_fr_list[1][temp_on_fr_list[1] < max_frame]\n",
    "right_stim_onsets = right_stim_onsets[right_stim_onsets > min_frame]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(convolved_stim_onset_fr[0][min_frame:max_frame], label='left')\n",
    "ax.plot(convolved_stim_onset_fr[1][min_frame:max_frame], label='right')\n",
    "ax.scatter(left_stim_onsets-min_frame, np.ones(left_stim_onsets.shape)*100, color='b')\n",
    "ax.scatter(right_stim_onsets-min_frame, np.ones(right_stim_onsets.shape)*100, color='orange')\n",
    "ax.plot(np.mean(neural_leaky_integrator[min_frame:max_frame], axis=1)*20*100, alpha=0.5, label='neural mean')\n",
    "plt.show()\n",
    "plt.legend(fontsize='x-small')\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}convolved-signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate/plot embeddings ###\n",
    "\n",
    "# # CEBRA's plot_embedding function is bugged\n",
    "fig = plt.figure(figsize=(6,12))\n",
    "ax1 = plt.subplot(211, projection='3d')\n",
    "ax2 = plt.subplot(212, projection='3d')\n",
    "ax1.view_init(elev=20., azim=45)\n",
    "ax2.view_init(elev=20., azim=45)\n",
    "\n",
    "# trained embedding\n",
    "embedding_train = cebra_stim_on_model.transform(neural_train)\n",
    "cebra.plot_embedding(embedding=embedding_train, embedding_labels=cont_train[:,0], ax=ax1, cmap='cool', title=\"Training Embedding\")\n",
    "\n",
    "# test embedding\n",
    "embedding_test = cebra_stim_on_model.transform(neural_test)\n",
    "cebra.plot_embedding(embedding=embedding_test, embedding_labels=cont_test[:,0], ax=ax2, cmap='cool', title=\"Test embedding\")\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot loss and temperature ###\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "cebra.plot_loss(cebra_stim_on_model, ax=ax1)\n",
    "cebra.plot_temperature(cebra_stim_on_model, ax=ax2)\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}loss-and-temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decode ###\n",
    "\n",
    "## set what should be predicted ##\n",
    "# this should be function input\n",
    "if TIME_SINCE_LAST or LEAKY_INTEGRATOR:\n",
    "    predicted_var = cont_train\n",
    "else:\n",
    "    predicted_var = discrete_train\n",
    "\n",
    "# if variable to be predicted is multidimensional, take only the first dimension for\n",
    "# decoding\n",
    "if predicted_var.ndim > 1:\n",
    "    predicted_var_subset = predicted_var[:,0]\n",
    "else:\n",
    "    predicted_var_subset = predicted_var\n",
    "\n",
    "decoder = cebra.KNNDecoder()\n",
    "\n",
    "decoder.fit(embedding_train, predicted_var_subset)\n",
    "\n",
    "# decide whether to predict on the same embedding as trained on, or on the test embedding\n",
    "if PREDICT_ON_TRAIN:\n",
    "    # changed to predict on the training embedding\n",
    "    predictions = decoder.predict(embedding_train)\n",
    "    # also change what is considered the \"test\" set. In this case it will be the training set\n",
    "    cont_test = cont_train\n",
    "else:\n",
    "    predictions = decoder.predict(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualise ###\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ax1.plot(cont_test[:,0], label='ground truth')\n",
    "# ax1.plot(predictions, label='prediction')\n",
    "ax1.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax2.plot(predictions, label='prediction')\n",
    "ax2.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "ax3.plot(cont_test[:,0], label='ground truth')\n",
    "ax3.plot(predictions, label='predictions', alpha=0.6)\n",
    "ax3.legend(loc='upper right', fontsize='small')\n",
    "\n",
    "# ax3.text(7100, 1.00, f\"f1 :{f1_score:.2f}\", fontsize='small')\n",
    "# ax3.text(7100, 0.9, f\"f1_len :{lenient_f1_score:.2f}\", fontsize='small')\n",
    "# ax3.text(7100, 0.75, f\"len-non-bi :{f1_score:.2f}\", fontsize='small')\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    # save this figure to the model folder\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### analyse predictions ###\n",
    "\n",
    "# This should really be: 'if predicted variable takes continuous values and not discrete'\n",
    "if LEAKY_INTEGRATOR:\n",
    "\n",
    "    stim_laterality = 0 # left\n",
    "\n",
    "    ## find smoothed predictions ##\n",
    "    # use a rolling average filter\n",
    "    kernel_size = 50\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "    data = np.copy(predictions)\n",
    "\n",
    "    # run filter\n",
    "    smoothed_data = np.convolve(data, kernel, mode='full')\n",
    "\n",
    "    # plot smoothed data \n",
    "    fig, axs = plt.subplots(2,1, figsize=(10,8))\n",
    "    axs[0].plot(data, label='predictions', alpha=0.6)\n",
    "    axs[0].plot(smoothed_data, label='smoothed')\n",
    "    axs[0].legend(loc='upper right', fontsize='small')\n",
    "\n",
    "\n",
    "    axs[1].plot(data, label='predictions', alpha=0.6)\n",
    "    axs[1].plot(smoothed_data, label='smoothed')\n",
    "    axs[1].plot(cont_test[:,stim_laterality], label='ground truth')\n",
    "\n",
    "    axs[1].legend(loc='upper right', fontsize='small')\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        # save this figure to the model folder\n",
    "        plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}\" + \\\n",
    "                    f\"smoothed_predictions\")\n",
    "\n",
    "    ## find stimulus-triggered average predictions ## \n",
    "\n",
    "    n_seconds = 40\n",
    "    fr = FRAMERATE\n",
    "    num_frames_post = n_seconds*fr\n",
    "    num_frames_pre = int(n_seconds*fr*0.2)\n",
    "    data = np.copy(predictions)\n",
    "\n",
    "    # find stim on times\n",
    "    chosen_stim_on_frames = stim_on_fr_list[stim_laterality]\n",
    "    if not PREDICT_ON_TRAIN: # use test set where relevant\n",
    "        chosen_stim_on_frames = chosen_stim_on_frames[chosen_stim_on_frames > split_idx]\n",
    "                                                      \n",
    "\n",
    "    # create an array of n seconds around the stimulus\n",
    "    peri_stim = np.zeros((chosen_stim_on_frames.size, num_frames_post+num_frames_pre))\n",
    "    for i in range(chosen_stim_on_frames.size):\n",
    "        stim_idx = chosen_stim_on_frames[i] - split_idx     # test-set-centric index\n",
    "        peri_stim[i] = data[stim_idx-num_frames_pre : stim_idx+num_frames_post]\n",
    "\n",
    "\n",
    "    # produce a mean average of this array\n",
    "    peri_stim_avg = np.mean(peri_stim, axis=0)\n",
    "\n",
    "    # plot\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,4))\n",
    "    axs[0].plot(peri_stim_avg)\n",
    "    axs[0].vlines(num_frames_pre, 0,120, color='black', linestyle='dashed')\n",
    "\n",
    "    # also plot a smoothed version\n",
    "    kernel_size = 10\n",
    "    kernel = np.ones(kernel_size)/kernel_size\n",
    "    smoothed_peri_stim_avg = np.convolve(peri_stim_avg, kernel)\n",
    "    axs[1].plot(smoothed_peri_stim_avg)\n",
    "    axs[1].vlines(num_frames_pre, 0,120, color='black', linestyle='dashed')\n",
    "\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        # save this figure to the model folder\n",
    "        plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}\" + \\\n",
    "                    f\"stim-triggered-mean_long-form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### quantify continuous predictions (MSE) ###\n",
    "\n",
    "# This should really be: 'if predicted variable takes continuous values and not discrete'\n",
    "if LEAKY_INTEGRATOR:\n",
    "\n",
    "    predictions_smoothed = smoothed_data[:-49]  # leave off extra datapoints created by kernel\n",
    "    ground_truth = cont_test[:,stim_laterality]\n",
    "\n",
    "    # root mean squared error between predictions and ground truth\n",
    "    def RMSE(predictions, ground_truth):\n",
    "        diff = predictions - ground_truth\n",
    "        diff_sq = np.square(diff)\n",
    "        MSE = np.mean(diff_sq)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "\n",
    "        return RMSE\n",
    "\n",
    "    # run eval\n",
    "    rmse = RMSE(predictions_smoothed, ground_truth)\n",
    "\n",
    "    # append output to file\n",
    "    eval_file = f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}\" + \\\n",
    "                            f\"RMSE.txt\"\n",
    "    with open(eval_file, 'a+') as f:\n",
    "        f.write(f\"{rmse}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lenient_predictions(test_data, prediction_data, stim_types, stim_length=22):\n",
    "    ''' find the indexes where a prediction was made, and copy the ground truth\n",
    "        for the relevant stimulus if this prediction lies within stimulus on '''\n",
    "    \n",
    "    leniency_range = stim_length*2\n",
    "    predictions_lenient = np.zeros(prediction_data.size)\n",
    "\n",
    "    stim_predictions = []\n",
    "    for stim in stim_types:\n",
    "        stim_predictions.append(np.where(np.isin(prediction_data, stim))[0])\n",
    "\n",
    "    for i in range(len(stim_predictions)):\n",
    "        # for each stimulus predicted\n",
    "        prediction = stim_predictions[i]\n",
    "\n",
    "        # for each index in current stimulus prediction\n",
    "        for idx in prediction:\n",
    "            # check if this is a correct prediction (ground-truth also the same stimulus category)\n",
    "            if test_data[idx] == stim_types[i]:\n",
    "                # for the leniency_range timepoints around the stimulus, copy any 1s from the ground truth to predictions\n",
    "                upper_bound, lower_bound = np.ceil(idx+leniency_range/2).astype(int), np.ceil(idx-leniency_range/2).astype(int)\n",
    "                predictions_lenient[lower_bound:upper_bound] = test_data[lower_bound:upper_bound]\n",
    "\n",
    "    return predictions_lenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate decoder performance - f1 score ###\n",
    "# Also provide a lenient f1-score (where any prediction of a stimulus within \n",
    "# the stimulus duration counts as a fully accurate predictions)\n",
    "\n",
    "\n",
    "## Define the test set ## \n",
    "\n",
    "# if cutting dead time, make sure to use the correct data\n",
    "if TRIAL_TIME:\n",
    "    # are we predicting on training data?\n",
    "    if PREDICT_ON_TRAIN:\n",
    "        # left_spot, right_spot = left_spot_trial[:split_idx], right_spot_trial[:split_idx]\n",
    "        left_spot, right_spot = np.concatenate(discrete_list_left[train_trials_idx]),  \\\n",
    "                                np.concatenate(discrete_list_right[train_trials_idx])\n",
    "        # # convert back to binary because right_spot_trial has been multiplied\n",
    "        # right_spot = np.multiply(right_spot, 0.5).astype(int)\n",
    "    else:\n",
    "        # left_spot, right_spot = left_spot_trial[split_idx:], right_spot_trial[split_idx:]\n",
    "        left_spot, right_spot = np.concatenate(discrete_list_left[test_trials_idx]),  \\\n",
    "                        np.concatenate(discrete_list_right[test_trials_idx])\n",
    "        # # convert back to binary because right_spot_trial has been multiplied\n",
    "        # right_spot = np.multiply(right_spot, 0.5).astype(int)\n",
    "    \n",
    "# if not cutting dead time, just load in stimulus masks\n",
    "else:\n",
    "    # are we predicting on training data?\n",
    "    if PREDICT_ON_TRAIN:\n",
    "        # for this case, set \"test\" subset of neural to just be the training subset \n",
    "        # index stim_on mask to restrict data to the test subset\n",
    "        left_spot, right_spot = stim_on_frames[:split_idx,0], stim_on_frames[:split_idx,1]\n",
    "    else:\n",
    "        # index stim_on mask to restrict data to the test subset\n",
    "        left_spot, right_spot = stim_on_frames[split_idx:,0], stim_on_frames[split_idx:,1]\n",
    "\n",
    "# combine left and right spot stim_on mask\n",
    "# identical stimulus values\n",
    "discrete_test_f1_binary = np.add(left_spot, right_spot)\n",
    "# different stimulus values\n",
    "discrete_test_f1 = np.add(left_spot, np.multiply(right_spot, 2))\n",
    "\n",
    "\n",
    "## convert predictions to be stimulus-agnostic for binary f1-score ## \n",
    "\n",
    "idx_to_replace = np.where(np.isin(predictions, 2))[0]\n",
    "predictions_binary = np.copy(predictions)\n",
    "predictions_binary[idx_to_replace] = 1\n",
    "\n",
    "\n",
    "## calculate lenient predictions ##\n",
    "\n",
    "predictions_binary_lenient = create_lenient_predictions(discrete_test_f1_binary, predictions_binary, stim_types=[1])\n",
    "predictions_multiclass_lenient = create_lenient_predictions(discrete_test_f1, predictions, stim_types=[1,2])\n",
    "\n",
    "\n",
    "## calculate binary f1-score ##\n",
    "\n",
    "f1_score = sklearn.metrics.f1_score(discrete_test_f1_binary, predictions_binary)\n",
    "\n",
    "\n",
    "## calculate lenient binary f1-score ##\n",
    "\n",
    "f1_score_lenient = sklearn.metrics.f1_score(discrete_test_f1_binary, predictions_binary_lenient)\n",
    "\n",
    "\n",
    "## calculate average multiclass f1-score ##\n",
    "\n",
    "f1_score_multiclass = sklearn.metrics.f1_score(discrete_test_f1, predictions, \n",
    "                                                         average=None)\n",
    "# find averages of the multiclass f1-score\n",
    "f1_score_multiclass_average_stim = (f1_score_multiclass[1] + f1_score_multiclass[2])/2\n",
    "f1_score_multiclass_average_all = (f1_score_multiclass[0] + f1_score_multiclass[1] + f1_score_multiclass[2])/3\n",
    "\n",
    "\n",
    "## calcuate lenient average multiclass f1-score ##\n",
    "\n",
    "f1_score_multiclass_lenient = sklearn.metrics.f1_score(discrete_test_f1, predictions_multiclass_lenient, \n",
    "                                                         average=None)\n",
    "# find averages of the lenient multiclass f1-score\n",
    "f1_score_multiclass_lenient_average_all = (f1_score_multiclass_lenient[0] + f1_score_multiclass_lenient[1])/2\n",
    "\n",
    "\n",
    "## report f1-score ##\n",
    "\n",
    "print(f\"Strict f1-score:  {f1_score:.3f}\\nLenient f1-score: {f1_score_lenient:.3f}\\n\"+\n",
    "      f\"Strict multiclass f1-score (stim-only avg): {f1_score_multiclass_average_stim:.3f}\\n\"+\n",
    "      f\"Strict multiclass f1-score (avg all): {f1_score_multiclass_average_all:.3f}\\n\"+\n",
    "      f\"Lenient multiclass f1-score (avg all): {f1_score_multiclass_lenient_average_all:.3f}\\n\")\n",
    "\n",
    "\n",
    "## save f1-score ##\n",
    "if SAVE_FIGS:\n",
    "    with open(f\"{model_dir}/v{MODEL_VER}_{SAVE_FIGS_LABEL}f1-score.txt\", 'w') as f:\n",
    "        f.write(f\"Strict f1-score:  {f1_score:.3f}\\n\")\n",
    "        f.write(f\"Lenient f1-score: {f1_score_lenient:.3f}\\n\")\n",
    "        f.write(f\"Strict multiclass f1-score (stim-only avg): {f1_score_multiclass_average_stim:.3f}\\n\")\n",
    "        f.write(f\"Strict multiclass f1-score (avg all): {f1_score_multiclass_average_all:.3f}\\n\")\n",
    "        f.write(f\"Lenient multiclass f1-score (avg all): {f1_score_multiclass_lenient_average_all:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### quick bar chart code ###\n",
    "\n",
    "f1_scores = [f1_score, f1_score_lenient, f1_score_multiclass_average_stim, f1_score_multiclass_average_all, f1_score_multiclass_lenient_average_all]\n",
    "f1_score_descriptions = ['Binary', 'Lenient binary', 'Multiclass (stim avg) ', \\\n",
    "                          'multiclass (all avg)', 'Lenient multiclass (avg all)']\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "ax.bar(x=np.arange(len(f1_scores)), height=f1_scores, tick_label=f1_score_descriptions)\n",
    "ax.tick_params(labelsize=8)\n",
    "plt.xticks(rotation=15, ha=\"right\")\n",
    "plt.show()\n",
    "\n",
    "# save\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(f\"{model_dir}/v{MODEL_VER}/v{MODEL_VER}_{SAVE_FIGS_LABEL}f1-score_bar-chart\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
