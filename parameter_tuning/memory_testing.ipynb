{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find local limit for data size in VRAM\n",
    "- Repeatedly train models (low iters) with incremented batch sizes\n",
    "- Find the minimum batch size to give a memory error (so far, 15000 timepoints, 10000 ROIs is fine)\n",
    "- Test in this notebook and then implement as memory_testing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cebra\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # global params\n",
    "\n",
    "# list of all data files\n",
    "dat_files = ['/media/storage/DATA/lfads_export/f1_221027.h5',\n",
    "             '/media/storage/DATA/lfads_export/f1_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f2_221103.h5',\n",
    "             '/media/storage/DATA/lfads_export/f3_221103.h5']\n",
    "\n",
    "global FILENAME\n",
    "global TIMESTEPS\n",
    "global ROIS\n",
    "global ITERS\n",
    "\n",
    "FILENAME = dat_files[0]\n",
    "TIMESTEPS = 15000\n",
    "ROIS = 10000\n",
    "ITERS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEBRA(conditional='time', learning_rate=0.0001, max_iterations=1000,\n",
      "      model_architecture='offset10-model', output_dimension=3,\n",
      "      temperature_mode='auto', time_offsets=10, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# # Define model\n",
    "\n",
    "cebra_time_model = cebra.CEBRA(\n",
    "    model_architecture='offset10-model',\n",
    "    device='cuda_if_available',\n",
    "    conditional='time',\n",
    "    temperature_mode='auto',\n",
    "    min_temperature=0.1,\n",
    "    time_offsets=10,\n",
    "    max_iterations=ITERS,                    # kept low for speed\n",
    "    max_adapt_iterations=500,\n",
    "    batch_size=None,\n",
    "    learning_rate=1e-4,\n",
    "    output_dimension=3,\n",
    "    verbose=True,\n",
    "    num_hidden_units=32,\n",
    "    hybrid=False\n",
    "    )\n",
    "print(cebra_time_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing data...\n",
      "Full neural dataset shape is: (43350, 93122)\n",
      "Truncated dataset shapes are:\n",
      "             neural: (15000, 10000)\n",
      "Data accessed.\n",
      "f1_221027_dfof.npz loaded.\n"
     ]
    }
   ],
   "source": [
    "# # Load data for a single fish\n",
    "\n",
    "# paths\n",
    "filename = FILENAME\n",
    "filename_trunc = filename.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = 'data/'\n",
    "filename_dfof = f'{filename[-12:-3]}_dfof.npz'\n",
    "\n",
    "# choose where in dataset to sample\n",
    "start, stop = 0, 0+TIMESTEPS\n",
    "\n",
    "# extract and neural data\n",
    "# do not attempt to load the entire file \n",
    "print(\"Accessing data...\")\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    \n",
    "    # neural \n",
    "    neural = f['rois']['dfof']\n",
    "    print(f\"Full neural dataset shape is: {neural.shape}\")\n",
    "\n",
    "    # select first TIMESTEPS timesteps and random ROIS rois\n",
    "    # neural\n",
    "    neural_indexes = np.sort(\n",
    "                        np.random.choice(\n",
    "                                    np.arange(neural.shape[1]), size=ROIS, replace=False\n",
    "                                    )\n",
    "                        )\n",
    "    neural = np.array(neural[start:stop, neural_indexes])\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Truncated dataset shapes are:\\n \\\n",
    "            neural: {neural.shape}\")\n",
    "\n",
    "    assert(neural.shape == (TIMESTEPS, ROIS))\n",
    "\n",
    "    # save dataset\n",
    "    np.savez(f'{data_folder}{filename_dfof}', neural=neural)\n",
    "\n",
    "    # load dataset\n",
    "    print(\"Data accessed.\")\n",
    "    neural = cebra.load_data(f'{data_folder}{filename_dfof}', key=\"neural\")\n",
    "    print(f\"{filename_dfof} loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incrementally load more data and fit models\n",
    "\n",
    "# paths\n",
    "filename = FILENAME\n",
    "filename_trunc = filename.split('/')[-1][:-3] # fish and date only\n",
    "data_folder = 'data/'\n",
    "filename_dfof = f'{filename[-12:-3]}_dfof.npz'\n",
    "filename_output = 'stdout_VRAM_test.txt'\n",
    "\n",
    "# params\n",
    "timesteps = 15000\n",
    "\n",
    "# flags\n",
    "train_model = False\n",
    "\n",
    "# redirect print output to text file\n",
    "orig_stdout = sys.stdout\n",
    "output = open(filename_output, 'w')\n",
    "sys.stdout = output\n",
    "\n",
    "\n",
    "# Incremented timesteps, constant ROIs\n",
    "while True:\n",
    "    try:\n",
    "        # choose where in dataset to sample\n",
    "        start, stop = 0, 0+timesteps\n",
    "\n",
    "        # extract and neural data\n",
    "        # do not attempt to load the entire file \n",
    "        print(\"Accessing data...\")\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            \n",
    "            # neural \n",
    "            neural = f['rois']['dfof']\n",
    "            \n",
    "            # select first TIMESTEPS timesteps and random ROIS rois\n",
    "            # neural\n",
    "            neural_indexes = np.sort(\n",
    "                                np.random.choice(\n",
    "                                            np.arange(neural.shape[1]), size=ROIS, replace=False\n",
    "                                            )\n",
    "                                )\n",
    "            neural = np.array(neural[start:stop, neural_indexes])\n",
    "            print(f\"Loaded neural dataset of shape: {neural.shape}\")\n",
    "\n",
    "            assert(neural.shape == (TIMESTEPS, ROIS))\n",
    "\n",
    "            # save dataset\n",
    "            np.savez(f'{data_folder}{filename_dfof}', neural=neural)\n",
    "            print(f\"Saved neural dataset of shape: {neural.shape}\")\n",
    "\n",
    "\n",
    "            # load dataset\n",
    "            neural = cebra.load_data(f'{data_folder}{filename_dfof}', key=\"neural\")\n",
    "            print(f\"Loaded neural dataset of shape {neural.shape}.\\n\")\n",
    "\n",
    "            \n",
    "            # train and save the model\n",
    "            if train_model:\n",
    "                model_name = f\"{filename_trunc}_time_{timesteps}points_{ROIS}rois_{ITERS}iters.pt\"\n",
    "                model_path = f'models/{model_name}'\n",
    "\n",
    "                cebra_time_model.fit(neural)\n",
    "                cebra_time_model.save(model_path)\n",
    "                print(f\"Model fit and saved for neural dataset of shape {neural.shape}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
